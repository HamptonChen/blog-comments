<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>程序员的修行之路</title>
  
  <subtitle>Hampton Chen&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-15T16:31:14.558Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Hampton Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>推荐系统介绍</title>
    <link href="http://yoursite.com/2019/04/16/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/04/16/推荐系统介绍/</id>
    <published>2019-04-15T16:29:20.000Z</published>
    <updated>2019-04-15T16:31:14.558Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>随着互联网的快速发展，我们进入一个信息爆炸的时代。互联网的发展，为我们提供了越来越多的服务平台，比如购物平台、视频播放网站、音乐播放器、社交网婚恋网等等，提供的物品种类也越来越多样。如何更好地满足客户的需求，成了企业的难题。</p><p>在这个信息爆炸的时代，无论是消费者还是信息生产者都遇到了很大的挑战：作为消费者，如何从大量信息中找到自己感兴趣的信息是一件非常困难的事情；作为信息生产者，如何让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。<br><a id="more"></a></p><h2 id="二、什么是推荐系统"><a href="#二、什么是推荐系统" class="headerlink" title="二、什么是推荐系统"></a>二、什么是推荐系统</h2><p>推荐系统是一项工程技术解决方案，通过利用机器学习等技术，在用户使用产品进行浏览交互的过程中，系统主动用户展示可能会喜欢的物品，从而促进物品的消费，节省用户时间，提升用户体验，做到资源的优化配置。</p><p>要将推荐系统落地到业务上需要大量的工程开发：涉及到日志打点、日志收集、ETL、分布式计算、特征工程、推荐算法建模、数据存储、提供接口服务、UI展示和交互、推荐效果评估等。</p><p>推荐系统的本质是在用户需求不明确的情况下，从海量的信息中为用户寻找其感兴趣的信息的技术手段。</p><p>推荐系统的任务就是联系用户和信息(物品)，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。</p><p>推荐系统很好满足了用户、平台、内容提供商三方的需求。以淘宝为例：用户及在淘宝上购物的买家，平台即淘宝网站，网站上众多的店主就是内容提供方。通过推荐系统可以更好将商品曝光给要购买的用户，提升社会资源的配置效率。</p><h2 id="三、推荐系统应用领域"><a href="#三、推荐系统应用领域" class="headerlink" title="三、推荐系统应用领域"></a>三、推荐系统应用领域</h2><p>推荐系统应用场景正在不断被挖掘和创造，只要存在大量供用户消费的产品，就有推荐系统发挥价值的地方。</p><p>推荐系统主要应用的领域有如下几类：</p><ul><li>电子商务：淘宝、京东、苏宁易购、亚马逊等</li><li>视频：腾讯视频、优酷、抖音等</li><li>音乐：网易云音乐、QQ音乐等</li><li>生活服务类：美团、携程</li><li>资讯类：头条、一点资讯等</li><li>社交类：陌陌、珍爱网等</li></ul><h2 id="四、常用推荐算法"><a href="#四、常用推荐算法" class="headerlink" title="四、常用推荐算法"></a>四、常用推荐算法</h2><p>接下来我们简单介绍一些推荐系统常用的算法：</p><h4 id="1、基于内容的推荐"><a href="#1、基于内容的推荐" class="headerlink" title="1、基于内容的推荐"></a>1、基于内容的推荐</h4><p>前面我们提到过推荐系统通过技术将用户和物品关联起来，物品本身包含很多属性，用户通过与物品的交互产生行为日志，这些日志可以作为衡量用户对物品偏好的标签，通过这些标签为用户做推荐，这就是基于内容的推荐算法。以音乐播放器推荐歌曲为例，歌曲有歌名、演唱者、类型、年代等标签信息，假设某个用户经常听张杰的歌，或者用户收藏的歌单中大部分都是张杰的歌曲，那么我们可以根据这些兴趣特征为用户推荐张杰的歌。</p><h4 id="2、基于协同过滤的推荐"><a href="#2、基于协同过滤的推荐" class="headerlink" title="2、基于协同过滤的推荐"></a>2、基于协同过滤的推荐</h4><p>用户与物品的交互留下了用户的标记，我们可以通过“物以类聚，人以群分”的思想为用户提供个性化推荐。</p><p>“物以类聚”具体来说就是：如果有许多用户对两个物品有相似的偏好，说明这两个物品是“相似”的，通过推荐与用户喜欢过的物品相似的物品的方法为用户提供个性化推荐，这就是基于物品的协同过滤推荐算法。</p><p>“人以群分”简单来说就是：找到与用户兴趣相同的人，将这些兴趣相同的用户浏览过的物品推荐给用户，这就是基于用户的协同过滤推荐算法。</p><h2 id="五、推荐系统的价值"><a href="#五、推荐系统的价值" class="headerlink" title="五、推荐系统的价值"></a>五、推荐系统的价值</h2><p>接下来我们简单介绍一下推荐系统的价值：</p><p>从<strong>用户</strong>角度来说，推荐系统可以让用户快速从海量信息中找到自己感兴趣的物品，节省了用户时间，提升用户的使用体验。<br>从<strong>平台</strong>角度来看：精准的推荐可以提升用户对平台的粘性，让用户喜欢上平台。平台整体营销收入提升，发现用户更多需求，满足其需求销售更多相关服务，获取更多利润。<br>从<strong>内容提供商</strong>角度来看，提升物品被卖出去的概率，提升提供商的销量。</p><p>文章参考来源：<a href="https://mp.weixin.qq.com/s/DofYtvZCe-7RTicLqYtL4A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/DofYtvZCe-7RTicLqYtL4A</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、背景&quot;&gt;&lt;a href=&quot;#一、背景&quot; class=&quot;headerlink&quot; title=&quot;一、背景&quot;&gt;&lt;/a&gt;一、背景&lt;/h2&gt;&lt;p&gt;随着互联网的快速发展，我们进入一个信息爆炸的时代。互联网的发展，为我们提供了越来越多的服务平台，比如购物平台、视频播放网站、音乐播放器、社交网婚恋网等等，提供的物品种类也越来越多样。如何更好地满足客户的需求，成了企业的难题。&lt;/p&gt;
&lt;p&gt;在这个信息爆炸的时代，无论是消费者还是信息生产者都遇到了很大的挑战：作为消费者，如何从大量信息中找到自己感兴趣的信息是一件非常困难的事情；作为信息生产者，如何让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。&lt;br&gt;
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="推荐系统" scheme="http://yoursite.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>中文分词</title>
    <link href="http://yoursite.com/2019/04/09/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    <id>http://yoursite.com/2019/04/09/中文分词/</id>
    <published>2019-04-09T02:52:01.000Z</published>
    <updated>2019-04-09T13:59:49.916Z</updated>
    
    <content type="html"><![CDATA[<p>中文分词(Chinese Word Segmentation)：是指将一个汉字序列切分为一个个单独的词。中文分词是中文自然语言处理的一个最基本的环节。中文分词与英文分词有很大的不同，对英文而言，一个单词就是一个词，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，需要人为切分。<br><a id="more"></a><br>根据中文分词的特点，可以把中文分词算法分为四大类：</p><ul><li>基于规则的分词方法</li><li>基于统计的分词方法</li><li>基于语义的分词方法</li><li>基于理解的分词方法</li></ul><hr><h2 id="基于规则的分词方法"><a href="#基于规则的分词方法" class="headerlink" title="基于规则的分词方法"></a>基于规则的分词方法</h2><p>该分词方法又称为机械分词方法、基于字典的分词方法。它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配。若在词典中找到某个字符串，则匹配成功。</p><p>该方法有三个要素：分词词典、文本扫描顺序和匹配原则。文本的扫描顺序有正向扫描、逆向扫描和双向扫描。匹配原则主要有最大匹配、最小匹配、逐词匹配和最佳匹配。</p><p><strong>最大匹配法（MM）</strong>：基本思想是：假设自动分词词典中的最长词条所含汉字的个数为 i，则取被处理材料当前字符串序列中的前 i 个字符作为匹配字段，查找分词词典，若词典中有这样一个 i 字词，则匹配成功，匹配字段作为一个词被切分出来；若词典中找不到这样的一个 i 字词，则匹配失败，匹配字段去掉最后一个汉字，剩下的字符作为新的匹配字段，再进行匹配，如此进行下去，直到匹配成功为止。</p><p><strong>逆向最大匹配法（RMM）</strong>：该方法的分词过程与 MM 法相同，不同的是从句子（或文章）末尾开始处理，每次匹配不成功时去掉的是前面的一个汉字。</p><p><strong>逐词遍历法</strong>：把词典中的词按照由长到短递减的顺序逐字搜索整个待处理的材料，一直到把全部的词切分出来为止。不论分词词典多大，被处理的材料多么小，都得把这个分词词典匹配一遍。</p><p><strong>设立切分标志法</strong>：切分标志有自然和非自然之分。自然切分标志是指文章中出现的非文字符号，如标点符号等；非自然标志是利用词缀和不构成词的词（包 括单音词、复音节词以及象声词等）。设立切分标志法首先收集众多的切分标志，分词时先找出切分标志，把句子切分为一些较短的字段，再用 MM、RMM 或其它的方法进行细加工。这种方法并非真正意义上的分词方法，只是自动分词的一种前处理方式而已，它要额外消耗时间扫描切分标志，增加存储空间存放那些非 自然切分标志。</p><p><strong>最佳匹配法（OM）</strong>：此法分为正向的最佳匹配法和逆向的最佳匹配法，其出发点是：在词典中按词频的大小顺序排列词条，以求缩短对分词词典的检索时 间，达到最佳效果，从而降低分词的时间复杂度，加快分词速度。实质上，这种方法也不是一种纯粹意义上的分词方法，它只是一种对分词词典的组织方式。OM 法的分词词典每条词的前面必须有指明长度的数据项，所以其空间复杂度有所增加，对提高分词精度没有影响，分词处理的时间复杂度有所降低。</p><p>基于规则的分词方法的优点是简单，易于实现。但缺点有很多：匹配速度慢；存在交集型和组合型歧义切分问题；词本身没有一个标准的定义，没有统一标准的词集；不同词典产生的歧义也不同；缺乏自学习的智能性。</p><hr><h2 id="基于统计的分词方法"><a href="#基于统计的分词方法" class="headerlink" title="基于统计的分词方法"></a>基于统计的分词方法</h2><p>基于统计的分词方法是在给定大量已经分词的文本情况下，利用统计机器学习模型学习词语切分的规律，从而实现对未知文本的切分。</p><p>该方法的主要思想：词是稳定的组合，在上下文中，相邻的字同时出现的次数越多，越有可能构成一个词。因此字与字相邻出现的概率能较好反映词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。</p><p>该方法应用的主要统计模型有：N元文法模型(N-gram)、隐马尔科夫模型(Hiden Markov Model, HMM)、最大熵模型(ME)、条件随机场模型(Conditional Random Fields, CRF)等。</p><p>在实际的应用中，这种分词方法都需要使用分词词典来进行字符串匹配分词，同时使用统计方法识别一些新词，即将字符串频率统计和字符串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。</p><hr><h2 id="基于语义的分词方法"><a href="#基于语义的分词方法" class="headerlink" title="基于语义的分词方法"></a>基于语义的分词方法</h2><p>语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。</p><p><strong>扩充转移网络法</strong>：该方法以有限状态机概念为基础。有限状态机只能识别正则语言，对有限状态机作的第一次扩充使其具有递归能力，形成递归转移网络 （RTN）。在RTN 中，弧线上的标志不仅可以是终极符（语言中的单词）或非终极符（词类），还可以调用另外的子网络名字分非终极符（如字或字串的成词条件）。这样，计算机在 运行某个子网络时，就可以调用另外的子网络，还可以递归调用。词法扩充转移网络的使用， 使分词处理和语言理解的句法处理阶段交互成为可能，并且有效地解决了汉语分词的歧义。</p><p><strong>矩阵约束法</strong>：其基本思想是：先建立一个语法约束矩阵和一个语义约束矩阵， 其中元素分别表明具有某词性的词和具有另一词性的词相邻是否符合语法规则， 属于某语义类的词和属于另一词义类的词相邻是否符合逻辑，机器在切分时以之约束分词结果。</p><hr><h2 id="基于理解的分词方法"><a href="#基于理解的分词方法" class="headerlink" title="基于理解的分词方法"></a>基于理解的分词方法</h2><p>基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。目前基于理解的分词方法主要有专家系统分词法和神经网络分词法等。</p><p><strong>专家系统分词法</strong>：从专家系统角度把分词的知识（包括常识性分词知识与消除歧义切分的启发性知识即歧义切分规则）从实现分词过程的推理机中独立出来，使知识库的维护与推理机的实现互不干扰，从而使知识库易于维护和管理。它还具有发现交集歧义字段和多义组合歧义字段的能力和一定的自学习功能。</p><p><strong>神经网络分词法</strong>：该方法是模拟人脑并行，分布处理和建立数值计算模型工作的。它将分词知识所分散隐式的方法存入神经网络内部，通过自学习和训练修改内部权值，以达到正确的分词结果，最后给出神经网络自动分词结果，如使用 LSTM、GRU 等神经网络模型等。</p><p><strong>神经网络专家系统集成式分词法</strong>：该方法首先启动神经网络进行分词，当神经网络对新出现的词不能给出准确切分时，激活专家系统进行分析判断，依据知识库进行推理，得出初步分析，并启动学习机制对神经网络进行训练。该方法可以较充分发挥神经网络与专家系统二者优势，进一步提高分词效率。</p><hr><p><strong>参考来源：</strong><br><a href="https://cuiqingcai.com/5844.html" target="_blank" rel="noopener">https://cuiqingcai.com/5844.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;中文分词(Chinese Word Segmentation)：是指将一个汉字序列切分为一个个单独的词。中文分词是中文自然语言处理的一个最基本的环节。中文分词与英文分词有很大的不同，对英文而言，一个单词就是一个词，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，需要人为切分。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="中文分词" scheme="http://yoursite.com/tags/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>文本挖掘预处理之TF-IDF</title>
    <link href="http://yoursite.com/2019/03/25/TF-IDF-1/"/>
    <id>http://yoursite.com/2019/03/25/TF-IDF-1/</id>
    <published>2019-03-25T14:10:10.000Z</published>
    <updated>2019-03-25T14:37:14.713Z</updated>
    
    <content type="html"><![CDATA[<p>TF-IDF（Term Frequency-Inverse Document Frequency）即“词频-反文档频率”，主要由TF和IDF两部分组成。TF-IDF是一种用于资讯检索与资讯探勘的常用加权技术，是一种统计方法，用于评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要程度与它在文件中出现的次数成正比，但同时与它在语料库中出现的频率成反比。<br><a id="more"></a></p><p><strong>TF——词频：</strong>一个词在文章中出现的次数。</p><p>在计算词频时，需要注意停用词的过滤。什么是停用词：在文章中出现次数最多的“的”、“是”、“在”等最常用词，但对结果毫无帮助，必须过滤的词。</p><p>TF计算有两种方式，具体公式如下：</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-1.png#pic_center" alt="Alt"><br><img src="/images/articles/2019/20190325TF-IDF-2.png#pic_center" alt="Alt"><br></div><p><strong>IDF——反文档频率：</strong>一个词在所有文章中出现的频率。如果包含这个词的文章越少，IDF越大，则说明词具有很好的类别区分能力。计算公式如下：</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-3.png#pic_center" alt="Alt"><br></div><p>将TF和IDF相乘，就得到一个词的TF-IDF值，某个词对文章的重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-4.png#pic_center" alt="Alt"><br></div><h2 id="TF-IDF总结："><a href="#TF-IDF总结：" class="headerlink" title="TF-IDF总结："></a>TF-IDF总结：</h2><p><strong>优点：</strong>简单快速，结果比较符合实际情况。</p><p><strong>缺点：</strong>单纯以“词频”做衡量标准，不够全面，有时重要的词可能出现的次数不多。</p><h2 id="用python实现TF-IDF的计算"><a href="#用python实现TF-IDF的计算" class="headerlink" title="用python实现TF-IDF的计算"></a>用python实现TF-IDF的计算</h2><p>将下图所示的已经分好词的文章作为语料库，计算101it.seg.cln.txt中的TF-IDF。</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-5.png#pic_center" alt="Alt"><br></div><p>具体python代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 要计算TF-IDF的文章路径</span></span><br><span class="line">file_path = <span class="string">'./data/101it.seg.cln.txt'</span></span><br><span class="line"><span class="comment"># 语料库目录路径</span></span><br><span class="line">data_dir_path = <span class="string">'./data'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 获取文章内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_content</span><span class="params">(file)</span>:</span></span><br><span class="line">    content = open(file, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 计算IDF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_idf</span><span class="params">(dir_path)</span>:</span></span><br><span class="line">    all_word_set = set()</span><br><span class="line">    article_list = []</span><br><span class="line">    article_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fd <span class="keyword">in</span> os.listdir(dir_path):</span><br><span class="line">        article_count += <span class="number">1</span></span><br><span class="line">        file = dir_path + <span class="string">'/'</span> + fd</span><br><span class="line">        content = read_content(file)</span><br><span class="line">        content_set = set()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> content:</span><br><span class="line">            word_tmp = line.strip().split(<span class="string">' '</span>)</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> word_tmp:</span><br><span class="line">                word = word.strip()</span><br><span class="line">                all_word_set.add(word)</span><br><span class="line">                content_set.add(word)</span><br><span class="line">        article_list.append(content_set)</span><br><span class="line"> </span><br><span class="line">    idf_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> all_word_set:</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> article <span class="keyword">in</span> article_list:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> article:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">        idf_dict[word] = math.log(float(article_count)/(float(count) + <span class="number">1.0</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> idf_dict</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 计算TF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_tf</span><span class="params">(file)</span>:</span></span><br><span class="line">    content = read_content(file)</span><br><span class="line">    word_set = set()</span><br><span class="line">    word_dict = &#123;&#125;</span><br><span class="line">    word_count = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 计算词频和文章总词数</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> content:</span><br><span class="line">        word_tmp = line.strip().split(<span class="string">' '</span>)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_tmp:</span><br><span class="line">            word = word.strip()</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_dict:</span><br><span class="line">                word_dict[word] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word_dict[word] += <span class="number">1</span></span><br><span class="line">            word_count += <span class="number">1</span></span><br><span class="line">            word_set.add(word)</span><br><span class="line">    <span class="comment"># 计算TF</span></span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> word_set:</span><br><span class="line">        word_dict[tmp] = float(word_dict[tmp])/float(word_count)</span><br><span class="line">    <span class="keyword">return</span> word_dict</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    idf_dict = calculate_idf(data_dir_path)</span><br><span class="line">    tf_dict = calculate_tf(file_path)</span><br><span class="line">    tfidf_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> tf_dict:</span><br><span class="line">        tfidf_dict[key] = tf_dict[key] * idf_dict[key]</span><br><span class="line">    print(tfidf_dict)</span><br></pre></td></tr></table></figure><h2 id="TF-IDF应用："><a href="#TF-IDF应用：" class="headerlink" title="TF-IDF应用："></a>TF-IDF应用：</h2><p>TF-IDF有下面几个应用，具体的实现后续文章再给大家介绍：</p><ul><li><p>提取文章的关键词</p></li><li><p>TF-IDF结合余弦相似度找相似文章</p></li><li><p>给文章自动生成摘要</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TF-IDF（Term Frequency-Inverse Document Frequency）即“词频-反文档频率”，主要由TF和IDF两部分组成。TF-IDF是一种用于资讯检索与资讯探勘的常用加权技术，是一种统计方法，用于评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要程度与它在文件中出现的次数成正比，但同时与它在语料库中出现的频率成反比。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TF-IDF" scheme="http://yoursite.com/tags/TF-IDF/"/>
    
  </entry>
  
  <entry>
    <title>K-近邻(KNN)算法</title>
    <link href="http://yoursite.com/2019/03/21/knn/"/>
    <id>http://yoursite.com/2019/03/21/knn/</id>
    <published>2019-03-21T09:04:17.000Z</published>
    <updated>2019-03-25T14:30:10.875Z</updated>
    
    <content type="html"><![CDATA[<p>K-近邻（KNN，K-Nearest Neighbor）算法是一种基本分类与回归方法，在机器学习分类算法中占有相当大的地位，既是最简单的机器学习算法之一，也是基于实例的学习方法中最基本的，又是最好的文本分类算法之一。<br><a id="more"></a><br>我们本篇文章只讨论分类问题的KNN算法。</p><h2 id="KNN算法概述"><a href="#KNN算法概述" class="headerlink" title="KNN算法概述"></a>KNN算法概述</h2><p>KNN是通过测量不同特征值之间的距离进行分类。</p><p>KNN算法思路：如果一个样本在特征空间中的k各最相似（即特征空间中最近邻）的样本中的大多数属于某一个类别，则该样本也属于该类别。其中k一个是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。</p><p>KNN算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。</p><p>KNN算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的模型。k值的选择、距离度量和分类决策规则是KNN算法的三个基本要素。</p><h2 id="KNN算法工作原理"><a href="#KNN算法工作原理" class="headerlink" title="KNN算法工作原理"></a>KNN算法工作原理</h2><p>KNN算法工作原理可描述为：<br>1、假设有一个带有标签的训练样本集，其中包含每条数据与所属分类的对应关系。</p><p>2、输入没有带标签的新数据后，计算新数据与训练样本集中每条数据的距离。</p><p>3、对求得的所有距离进行升序排序。</p><p>4、选取k个与新数据距离最小的训练数据。</p><p>5、确定k个训练数据所在类别出现的频率。</p><p>6、返回k个训练数据中出现频率最高的类别作为新数据的分类。</p><h3 id="KNN算法距离计算方式"><a href="#KNN算法距离计算方式" class="headerlink" title="KNN算法距离计算方式"></a>KNN算法距离计算方式</h3><p>在KNN中，通过计算对象间距离来作为各个对象之间的相似性指标，这里的距离计算一般采用欧式距离或者是曼哈顿距离，两种距离的计算公式如下图所示：<br><img src="/images/articles/2019/20190321knn-1.png#pic_center" alt="Alt"></p><h3 id="KNN算法特点："><a href="#KNN算法特点：" class="headerlink" title="KNN算法特点："></a>KNN算法特点：</h3><ul><li><p>优点：精度高、对异常值不敏感、无数据输入假定</p></li><li><p>缺点：计算复杂度高、空间复杂度高</p></li><li><p>适用数据范围：数值型和标称型</p></li></ul><h2 id="KNN算法demo实例"><a href="#KNN算法demo实例" class="headerlink" title="KNN算法demo实例"></a>KNN算法demo实例</h2><p>python（python3.6）实现一个KNN算法的简单demo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建样本数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group = np.array([[<span class="number">1.0</span>, <span class="number">1.1</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0</span>, <span class="number">0.1</span>], [<span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 近邻算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span> <span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    diffMat = np.tile(inX,(dataSet.shape[<span class="number">0</span>],<span class="number">1</span>)) - dataSet <span class="comment">#待分类的输入向量与每个训练数据做差</span></span><br><span class="line">    distance = ((diffMat ** <span class="number">2</span>).sum(axis=<span class="number">1</span>)) ** <span class="number">0.5</span> <span class="comment">#欧氏距离</span></span><br><span class="line">    sortDistanceIndices = distance.argsort() <span class="comment">#从小到大的顺序，返回对应索引值</span></span><br><span class="line">    votelabel = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        votelabel.append(labels[sortDistanceIndices[i]])</span><br><span class="line">    Xlabel = Counter(votelabel).most_common(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Xlabel[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># # 创建数据集和 k-近邻算法</span></span><br><span class="line">    group, labels = createDataSet()</span><br><span class="line">    <span class="comment"># 新数据为[0, 0], k=3</span></span><br><span class="line">    label = classify0([<span class="number">0</span>, <span class="number">0</span>], group, labels, <span class="number">3</span>)</span><br><span class="line">    print(label)</span><br></pre></td></tr></table></figure><p>执行上面代码，在控制台打印输出  B，即为数据[0, 0]的分类。</p><h3 id="方法说明"><a href="#方法说明" class="headerlink" title="方法说明"></a>方法说明</h3><p>上面python代码中有几个方法在这里简单说明一下：</p><ul><li><p>Counter(votelabel).most_common(1)：求votelabel中出现次数最多的元素</p></li><li><p>np.tile(A,B)：若B为int型：在列方向上将A重复B次 若B为元组(m,n):将A在列方向上重复n次，在行方向上重复m次</p></li><li><p>sum(axis=1)：函数的axis参数，axis=0:按列相加；axis=1:按行的方向相加，即每行数据求和</p></li><li><p>argsort：将数组的值按从小到大排序后，输出索引值</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;K-近邻（KNN，K-Nearest Neighbor）算法是一种基本分类与回归方法，在机器学习分类算法中占有相当大的地位，既是最简单的机器学习算法之一，也是基于实例的学习方法中最基本的，又是最好的文本分类算法之一。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="KNN" scheme="http://yoursite.com/tags/KNN/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
