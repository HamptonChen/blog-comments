<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>代码视界</title>
  
  <subtitle>Hampton Chen&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.chenhanpeng.com/"/>
  <updated>2019-06-11T15:29:26.714Z</updated>
  <id>http://www.chenhanpeng.com/</id>
  
  <author>
    <name>Hampton Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow基础概念</title>
    <link href="http://www.chenhanpeng.com/2019/06/11/TensorFlow%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
    <id>http://www.chenhanpeng.com/2019/06/11/TensorFlow基础概念/</id>
    <published>2019-06-11T15:25:14.000Z</published>
    <updated>2019-06-11T15:29:26.714Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。</p><p>TensorFlow 是Google第二代大规模分布式深度学习框架。</p><ul><li>灵活通用的深度学习库</li><li>端云结合的人工智能引擎</li><li>高性能的基础平台软件</li><li>跨平台的机器学习系统</li></ul><p><strong>应用场景：</strong></p><p>行人检测、人脸检测、行为识别、身份证自动输入与人脸图像比较、OCR+自动化审核</p><h2 id="TensorFlow-数据流图介绍"><a href="#TensorFlow-数据流图介绍" class="headerlink" title="TensorFlow 数据流图介绍"></a>TensorFlow 数据流图介绍</h2><p>TensorFlow数据流图是一种声明式编程范式</p><p>声明式编程与命令式编程的多角度对比</p><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/TensorFlow/tensorflow-base-liutu-1.png" alt="Alt"><br></div><p>数据流图由有向边和节点组成</p><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/TensorFlow/tensorflow-base-liutu-2.png" alt="Alt"><br></div><p>数据流图的优势：<strong>快</strong></p><ul><li>并行计算快</li><li>分布式计算快(CPUs,GPUs,TPUs)</li><li>预编译优化(XLA)</li><li>可移植性好(Language-independent representation)</li></ul><h2 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量(Tensor)"></a>张量(Tensor)</h2><p>在数学里，张量是一种几何实体，广义上表示任意形式的“数据”。张量可以理解为0阶（rank）标量、1阶向量和2阶矩阵在高维度空间上的推广，张量的阶描述它表示数据的最大维度。</p><p>在TensorFlow中，张量表示某种相同的数据类型的多维数组<br>因此，张量有两个重要的属性：</p><p>1、<strong>数据类型</strong>，如浮点型、整型、字符串</p><p>2、<strong>数组形状</strong>： 各个维度的大小</p><p>TensorFlow张量是什么可以总结为一下几点：</p><ul><li>张量是用来表示多维数据的</li><li>张量是执行操作时的输入或输出数据</li><li>用户通过执行操作来创建或计算张量</li><li>张量的形状不一定在编译时确定，可以在运行时通过形状推断计算得到</li></ul><p>几类比较特别的张量，由以下操作产生：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf<span class="selector-class">.constant</span>     <span class="comment">// 常量</span></span><br><span class="line">tf<span class="selector-class">.placeholder</span>   <span class="comment">// 占位符</span></span><br><span class="line">tf<span class="selector-class">.Variable</span>      <span class="comment">// 变量</span></span><br></pre></td></tr></table></figure></p><h2 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量(Variable)"></a>变量(Variable)</h2><p>变量Variable是一种特殊的张量，主要作用是维护特定节点的状态，如深度学习或机器学习的模型参数。</p><p>tf.Variable方法是操作，返回值是变量（特殊张量）</p><p>通过tf.Variable方法创建的变量，与张量一样，可以作为操作的输入和输出。</p><p>不同之处：</p><ul><li>张量的生命周期通常随依赖的计算完成而结束，内存也随即释放</li><li>变量则常驻内存，在每一步训练时不断更新值，以实现模型参数的更新。</li></ul><h3 id="TensorFlow变量使用流程"><a href="#TensorFlow变量使用流程" class="headerlink" title="TensorFlow变量使用流程"></a>TensorFlow变量使用流程</h3><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/TensorFlow/tensorflow-base-variable-1.png" alt="Alt"><br></div><h2 id="操作-Operation"><a href="#操作-Operation" class="headerlink" title="操作(Operation)"></a>操作(Operation)</h2><p>TensorFlow用数据流图表示算法模型。数据流图由节点和有向边组成，每个节点均对应一个具体的操作。因此，操作是模型功能的<strong>实际载体</strong>。</p><p>数据流图中的节点按照功能不同可以分为3种：</p><ul><li><strong>存储节点</strong>：有状态的变量操作，通常用来存储模型参数</li><li><strong>计算节点</strong>：无状态的计算或控制操作，主要负责算法逻辑表达或流程控制。</li><li><strong>数据节点</strong>：数据的占位符操作，用于描述图外输入数据的属性。</li></ul><p><strong>操作的输入和输出是张量或操作（函数式编程）</strong></p><h3 id="TensorFlow典型计算和控制操作"><a href="#TensorFlow典型计算和控制操作" class="headerlink" title="TensorFlow典型计算和控制操作"></a>TensorFlow典型计算和控制操作</h3><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/TensorFlow/tensorflow-base-operation-1.png" alt="Alt"><br></div><h3 id="TensorFlow占位符操作"><a href="#TensorFlow占位符操作" class="headerlink" title="TensorFlow占位符操作"></a>TensorFlow占位符操作</h3><p>tensorflow 使用占位符操作表示图外输入的数据，如训练数据和测试数据。</p><p>TensorFlow数据流图描述了算法模型的计算拓扑，其中的各个操作(节点)都是抽象的函数映射或数学表达式。</p><p>换句话说，数据流图本身是一个具有计算拓扑和内部结构的“壳”。在用户向数据流图填充数据前，图中并没有真正执行任何计算。</p><h2 id="会话-Session"><a href="#会话-Session" class="headerlink" title="会话(Session)"></a>会话(Session)</h2><p>会话提供了估算张量和执行操作的运行环境，它是发放计算任务的客户端，所有计算任务都由它连接的执行引擎完成。一个会话的典型使用流程分为以下3步：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、创建会话</span></span><br><span class="line"><span class="comment"># target:会话连接的执行引擎  graph:会话加载的数据流图  config:会话启动时的配置项</span></span><br><span class="line">sess = tf.Session(<span class="attribute">target</span>=..., <span class="attribute">graph</span>=..., <span class="attribute">config</span>=...)</span><br><span class="line"><span class="comment"># 2、估算张量或执行操作</span></span><br><span class="line">sess.<span class="builtin-name">run</span>(<span class="built_in">..</span>.)</span><br><span class="line"><span class="comment"># 3、关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p><h3 id="会话执行"><a href="#会话执行" class="headerlink" title="会话执行"></a>会话执行</h3><p>获取张量值的另外两种方法：估算张量(Tensor.eval)与执行操作(Operation.run)</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"># 创建数据流图：<span class="keyword">y</span>=<span class="keyword">w</span> * <span class="keyword">x</span> + <span class="keyword">b</span>, 其中<span class="keyword">w</span>和<span class="keyword">b</span>为存储节点，<span class="keyword">x</span>为数据节点</span><br><span class="line"><span class="keyword">x</span> = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32)</span><br><span class="line"><span class="keyword">w</span> = <span class="keyword">tf</span>.Variable(<span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">b</span> = <span class="keyword">tf</span>.Variable(<span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">y</span> = <span class="keyword">w</span> * <span class="keyword">x</span> + <span class="keyword">b</span></span><br><span class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">    <span class="keyword">tf</span>.global_variables_initializer().run() # Operation.run</span><br><span class="line">    fetch = <span class="keyword">y</span>.<span class="built_in">eval</span>(feed_dict=&#123;<span class="keyword">x</span>: <span class="number">3.0</span>&#125;) # Tensor.<span class="built_in">eval</span></span><br><span class="line">    <span class="keyword">print</span>(fetch)   # fetch = <span class="number">1.0</span> * <span class="number">3.0</span> + <span class="number">1.0</span></span><br></pre></td></tr></table></figure><h3 id="会话执行原理"><a href="#会话执行原理" class="headerlink" title="会话执行原理"></a>会话执行原理</h3><p>当我们调用sess.run(train_op)语句执行训练操作时：</p><ul><li>首先，程序内部提取操作依赖的所有前置操作。这些操作的节点共同组成一幅子图。</li><li>然后，程序会将子图中的计算节点、存储节点和数据节点按照各自的执行设备分类，相同设备上的节点组成了一幅局部图</li><li>最后，每个设备上的局部图在实际执行时，根据节点间的依赖关系将各个节点有序地加载到设备上执行。</li></ul><h2 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器(Optimizer)"></a>优化器(Optimizer)</h2><p>优化器是实现优化算法的载体。</p><p>一次典型的迭代优化应该分为以下3个步骤：</p><p>1、<strong>计算梯度</strong>：调用compute_gradients方法;</p><p>2、<strong>处理梯度</strong>：用户按照自己需求处理梯度值，如梯度裁剪和梯度加权等。</p><p>3、<strong>应用梯度</strong>：调用apply_gradients方法，将处理后的梯度值应用到模型参数。</p><p>TensorFlow内置优化器：</p><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/TensorFlow/tensorflow-base-optimizer.png" alt="Alt"><br></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。&lt;/p&gt;
&lt;p&gt;TensorFlow 是Google第二代大规模分布式深度学习框架。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;灵活通用的深度学习库&lt;/li&gt;
&lt;li&gt;端云结合的人工智能引
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.chenhanpeng.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TensorFlow" scheme="http://www.chenhanpeng.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>CF-基于协同过滤的推荐算法</title>
    <link href="http://www.chenhanpeng.com/2019/05/13/CF-%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    <id>http://www.chenhanpeng.com/2019/05/13/CF-基于协同过滤的推荐算法/</id>
    <published>2019-05-13T15:05:11.000Z</published>
    <updated>2019-05-13T15:05:25.652Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>上一篇文章我们介绍了CB推荐算法，本篇文章我们将介绍另外一种推荐算法——基于协同过滤的推荐算法(Collaborative Filtering Recommendations)，下文我们统一简称为CF算法。</p><p>协同过滤推荐算法作为推荐算法中最经典的类型，包括在线的协同和离线的过滤两部分。在线协同是指通过在线数据找到用户可能喜欢的物品，离线过滤则是过滤掉一些不值得推荐的数据，比如推荐评分低的，或者推荐评分高但用户已经购买过的数据。</p><p>CF算法的数据源是基于用户历史行为和物品的矩阵数据，即UI（User-Item）矩阵数据。CF算法一般可以分为基于用户（User-Based）的协同过滤和基于物品（item-based）的协同过滤。</p><h1 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h1><h2 id="1、-User-Based-CF"><a href="#1、-User-Based-CF" class="headerlink" title="1、 User-Based CF"></a>1、 User-Based CF</h2><p>假设：</p><ul><li>用户喜欢跟他过去喜欢的物品相似的物品</li><li>历史上相似的物品在未来也相似</li></ul><p>方法：</p><ul><li>给定用户u，找到他过去喜欢的物品的集合R(u)</li><li>把和R(u)相似的物品推荐给u</li></ul><h2 id="2、-Item-Based-CF"><a href="#2、-Item-Based-CF" class="headerlink" title="2、 Item-Based CF"></a>2、 Item-Based CF</h2><p>假设：</p><ul><li>用户喜欢跟他过去喜欢的物品相似的物品</li><li>历史上相似的物品在未来也相似</li></ul><p>方法：</p><ul><li>给定用户u，找到他过去喜欢的物品的集合R(u)</li><li>把和R(u)相似的物品推荐给u</li></ul><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/CF_1.png" alt="Alt"><br></div><h1 id="CF算法优缺点"><a href="#CF算法优缺点" class="headerlink" title="CF算法优缺点"></a>CF算法优缺点</h1><p><strong>优点：</strong></p><ul><li>充分利用群体智慧</li><li>推荐精度高于CB</li><li>利于挖掘隐含的相关性</li></ul><p><strong>缺点：</strong></p><ul><li>推荐结果解释性较差</li><li>对时效性强的Item不适用</li><li>冷启动问题</li></ul><h1 id="算法处理过程"><a href="#算法处理过程" class="headerlink" title="算法处理过程"></a>算法处理过程</h1><h3 id="1、数据准备"><a href="#1、数据准备" class="headerlink" title="1、数据准备"></a>1、数据准备</h3><p>用户user_id,物品item_id，打分score（score可以是用户对某件物品的评分，或是根据用户行为计算出的偏好度得分，比如曝光、点击、收藏的加权得分，具体权重可以参考漏斗模型），数据如下：</p><table><thead><tr><th>user_id</th><th>item_id</th><th>score</th></tr></thead><tbody><tr><td>id1</td><td>item1</td><td>3</td></tr><tr><td>id1</td><td>item2</td><td>2</td></tr><tr><td>id2</td><td>item1</td><td>4</td></tr><tr><td>id2</td><td>item2</td><td>3</td></tr></tbody></table><h3 id="2、计算相似性矩阵"><a href="#2、计算相似性矩阵" class="headerlink" title="2、计算相似性矩阵"></a>2、计算相似性矩阵</h3><p>CF算法的关键在于计算获得user或item的相似度矩阵，即UU矩阵和II矩阵。</p><p><strong>User-Based：</strong></p><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/CF_2.png" alt="Alt"><br></div><p>用户之间的相似度计算，是基于对相同的物品打过分，可以将各个分值联合起来作为一个向量，然后计算余弦相似度：</p><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/CF_3.jpg" alt="Alt"><br></div><p><strong>Item-Based：</strong></p><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/CF_5.png" alt="Alt"><br></div><p>计算各个Item之间的相似度矩阵，即对两个Item都打过分的id的打分情况作为向量，同理得到item的相似度矩阵。</p><h3 id="3、推荐"><a href="#3、推荐" class="headerlink" title="3、推荐"></a>3、推荐</h3><p>根据相似度矩阵，选择与目标用户相似度最高的几位用户，在第一张表中选取各自打分较高的物品，形成一个推荐候选集合，准备推荐给目标用户。</p><h1 id="User-Based-CF和Item-Based-CF区别"><a href="#User-Based-CF和Item-Based-CF区别" class="headerlink" title="User-Based CF和Item-Based CF区别"></a>User-Based CF和Item-Based CF区别</h1><p>通过两种方法，我们发现两种的分数不一样，那么该用哪个呢，哪个真实，其实这个不重要，生活中我们一般是基于用户给用户推荐Top问题，而不是打分情况，即只要排好序就可以，工作这个分数其实还是有用的，一般我们有这么个准则，哪个维度小用哪个，电商网站物品的矩阵远大于用户矩阵，规模太大有时候造成一些慢，相反一样 </p><p>那么我们来看一下这两个对比不同</p><table><thead><tr><th></th><th>User-Based</th><th>Item-Based</th></tr></thead><tbody><tr><td>性能</td><td>适用用户较少场合，如果用户多，计算用户相似矩阵代价太大</td><td>适用于物品数明显小于用户数的场合，如果物品很多，计算物品相似度矩阵代价很大</td></tr><tr><td>领域</td><td>时效性强，用户个性化兴趣不太明显的领域</td><td>长尾物品丰富，用户个性化需求强烈的领域</td></tr><tr><td>实时性</td><td>用户有新行为，不一定造成推荐结果立即变化</td><td>用户有新行为，一定会导致推荐结果的实时变化</td></tr><tr><td>冷启动</td><td>在新用户对很少的物品产生行为后，不能立即对他进行个性化推荐，因为用户相似度表是每隔一段时间离线计算的  新物品上线后一段时间，一旦有用户对物品产生行为，就可以将新物品推荐给对它产生行为的用户兴趣相似的其他用户</td><td>新用户只要对一个物品产生行为，就可以给他推荐和该物品相关的其他物品   但没有办法在不离线更新物品相似度表的情况下将新物品推荐给用户</td></tr><tr><td>推荐理由</td><td>很难提供令用户信服的推荐解释</td><td>利用用户的历史行为给用户做推荐解释，可以令用户比较信服</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;上一篇文章我们介绍了CB推荐算法，本篇文章我们将介绍另外一种推荐算法——基于协同过滤的推荐算法(Collaborative Filterin
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.chenhanpeng.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="推荐算法" scheme="http://www.chenhanpeng.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="大数据" scheme="http://www.chenhanpeng.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>CB-基于内容的推荐算法</title>
    <link href="http://www.chenhanpeng.com/2019/05/07/CB-%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    <id>http://www.chenhanpeng.com/2019/05/07/CB-基于内容的推荐算法/</id>
    <published>2019-05-07T15:57:25.000Z</published>
    <updated>2019-05-07T16:03:18.488Z</updated>
    
    <content type="html"><![CDATA[<p>在推荐系统领域，一般主要有两种推荐任务：评分预测和Top-N推荐。下面我们简单介绍一下这两种推荐任务：</p><h3 id="评分预测："><a href="#评分预测：" class="headerlink" title="评分预测："></a>评分预测：</h3><p>我们以音乐推荐系统为例，首先用户A和用户B都对某几首歌进行喜欢程度打分，假设两个用户对周杰伦的《稻香》和陈奕迅的《好久不见》有相同的喜欢程度，且打分都不低，那么我们是否可以预测这两个用户有相同的爱好，那么我们可以将用户A喜欢的《青花瓷》这首歌推荐给用户B。</p><h3 id="Top-N推荐"><a href="#Top-N推荐" class="headerlink" title="Top-N推荐"></a>Top-N推荐</h3><p>假设用户A喜欢的音乐列表里有《十年》、《好久不见》、《稻香》，用户B喜欢的音乐列表里有《同桌的你》、《三国杀》、《逆战》，假设用户C刚开始使用该系统，并将《稻香》添加到喜欢的音乐中，那么我们是不是可以先推荐与用户C喜好相近的用户A喜欢的音乐给用户C，再推荐用户B喜欢的音乐。先推荐关联性高的，将关联性低的放在后面，这就是Top-N推荐。</p><h1 id="基于内容的推荐算法-CB"><a href="#基于内容的推荐算法-CB" class="headerlink" title="基于内容的推荐算法(CB)"></a>基于内容的推荐算法(CB)</h1><p>今天要介绍的CB(Content-Based Recommendations)算法是众多推荐推荐算法中的一种，也是比较早被使用的一种推荐算法。</p><h2 id="1、引入Item属性的基于内容的推荐"><a href="#1、引入Item属性的基于内容的推荐" class="headerlink" title="1、引入Item属性的基于内容的推荐"></a>1、引入Item属性的基于内容的推荐</h2><p>用户喜欢歌曲A(item)，现在有一首新歌曲B，如何确定是否要推荐给用户？</p><ul><li><p>首先我们对所有的音乐进行内容分析和item内容属性索引，即进行特征建立和建模，音乐的特征有：作者、年代、音乐风格等等。</p></li><li><p>然后我们去计算歌曲A和歌曲B两者的相关性，如何相关性高，则将歌曲B推荐给用户。</p></li></ul><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/CB-item.png" alt="Alt"><br></div><p>这类推荐算法有以下优缺点：</p><p><strong>优点：</strong></p><ul><li>提升推荐结果的相关性</li><li>结果可解释</li><li>推荐结果容易被用户感知</li></ul><p><strong>缺点：</strong></p><ul><li>无个性化</li><li>依赖于对item的深度分析</li></ul><h2 id="2、引入User属性的基于内容的推荐"><a href="#2、引入User属性的基于内容的推荐" class="headerlink" title="2、引入User属性的基于内容的推荐"></a>2、引入User属性的基于内容的推荐</h2><p>引入User属性的基于内容推荐算法加入了用户行为分析和建立用户兴趣模型。假设用户在过去的一段时间里听了爵士乐、DJ、周杰伦的歌，我们可以根据这几个历史标签进行数据建模，建立历史标签的正排倒排的内容分析。与上面不同的是这方面是基于用户的历史行为作分析，我们将其存到数据库，以后为用户推荐音乐时，先查库，然后进行相关的推荐。</p><div style="display: flex; justify-content: center;"><br><img src="/images/articles/2019/CB-user.png" alt="Alt"><br></div><p>这类推荐算法有以下优缺点：</p><p><strong>优点：</strong></p><ul><li>用户模型刻画了用户兴趣需求</li><li>推荐形式多样，具有个性化</li><li>结果可解释</li></ul><p><strong>缺点：</strong></p><ul><li>推荐精度低</li><li>马太效应</li><li>用户行为稀疏导致覆盖率低</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在推荐系统领域，一般主要有两种推荐任务：评分预测和Top-N推荐。下面我们简单介绍一下这两种推荐任务：&lt;/p&gt;
&lt;h3 id=&quot;评分预测：&quot;&gt;&lt;a href=&quot;#评分预测：&quot; class=&quot;headerlink&quot; title=&quot;评分预测：&quot;&gt;&lt;/a&gt;评分预测：&lt;/h3&gt;&lt;p
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.chenhanpeng.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="推荐算法" scheme="http://www.chenhanpeng.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="大数据" scheme="http://www.chenhanpeng.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Vue-CLI3 环境变量和模式</title>
    <link href="http://www.chenhanpeng.com/2019/04/29/vue-cli3-%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E6%A8%A1%E5%BC%8F/"/>
    <id>http://www.chenhanpeng.com/2019/04/29/vue-cli3-环境变量和模式/</id>
    <published>2019-04-29T14:46:30.000Z</published>
    <updated>2019-06-06T08:38:53.455Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间工作中用Vue-CLI3构建的Vue工程一些静态资源，比如静态H5页面、图片、图标等等，我们一般放在固定的一些服务器上，链接前缀一般相对固定，但我们打包发布一般要区分测试环境和生产环境，此时的静态资源路径也需要区分测试和生产，如果每次打包都要根据部署的环境去修改路径十分麻烦，这时候vue-cli的模式和环境变量则能够很好地解决这个麻烦。</p><h2 id="模式"><a href="#模式" class="headerlink" title="模式"></a>模式</h2><p><strong>模式</strong>是Vue CLI项目中一个重要的概念。默认情况下，一个Vue CLI项目有三个模式：</p><ul><li><p>development 模式用于vue-cli-service serve</p></li><li><p>production 模式用于vue-cli-service build和vue-cli-service test:e2e</p></li><li><p>test 模式用于vue-cli-service test:unit</p></li></ul><p>模式不同于NODE_ENV，一个模式可以包含多个环境变量。每个模式都会将NODE_ENV的值设置为模式的名称，比如：development模式下NODE_ENV的值会被设置为development</p><p>当然，我们也可以通过为.env文件增加后缀来设置某个模式下特有的环境变量。</p><h4 id="示例：test模式"><a href="#示例：test模式" class="headerlink" title="示例：test模式"></a>示例：test模式</h4><p>我们在项目根目录下创建一个名为.env.test和.env的文件</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">//</span> <span class="string">.env</span>文件：</span><br><span class="line">VUE_APP_TITLE=VUE-CLI3-DEMO</span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// .env.test文件</span><br><span class="line">NODE_ENV=production</span><br><span class="line">VUE_APP_TITLE=VUE-CLI3-DEMO(test)</span><br></pre></td></tr></table></figure><ul><li><p>vue-cli-service build 会加载可能存在的 .env、.env.production 和 .env.production.local 文件然后构建出生产环境应用；</p></li><li><p>vue-cli-service build –mode test 会在 staging 模式下加载可能存在的 .env、.env.test 和 .env.test.local 文件然后构建出生产环境应用。</p></li></ul><p>这两种情况下，根据 NODE_ENV，构建出的应用都是生产环境应用，但是在 test 版本中，process.env.VUE_APP_TITLE 被覆写成了另一个值。</p><p>我们在vue.config.js文件中添加console.log(process.env.VUE_APP_TITLE)</p><p>在package.json文件中添加”build-test”: “vue-cli-service build –mode test”</p><p>执行npm run build和npm run build-test</p><p>通过查看控制台打印出的分别是VUE-CLI3-DEMO和VUE-CLI3-DEMO(test)，由此可见不同模式下的环境变量不同。</p><hr><h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>只有以<strong>VUE_APP_</strong>开头的变量才会被webpack.DefinePlugin静态嵌入到客户端侧的包中，我们可以在代码中以下面的方式访问：process.env.VUE_APP_TITLE</p><p>除了VUE_APP_*变量外，还有两个始终可用的特殊变量：</p><ul><li><strong>NODE_ENV</strong>  值为development、productin、test中的一个。</li><li><strong>BASE_URL</strong>  与vue.config.js中的publicPath相符，即应用部署的基础路径</li></ul><hr><p>回到我们最开始的关于静态资源在不同环境下的路径问题，我们可以分别创建.env.test和.env.production两个文件，在文件中添加变量VUE_APP_STATIC_BASE_URL，根据不同环境赋予不同的值，在代码中用到静态资源的时候通过process.env.VUE_APP_STATIC_BASE_URL + 静态资源后续具体路径，再package.json中添加相应的模式打包命令，这样就可以比较好解决我们最开始提出的问题了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前段时间工作中用Vue-CLI3构建的Vue工程一些静态资源，比如静态H5页面、图片、图标等等，我们一般放在固定的一些服务器上，链接前缀一般相对固定，但我们打包发布一般要区分测试环境和生产环境，此时的静态资源路径也需要区分测试和生产，如果每次打包都要根据部署的环境去修改路径
      
    
    </summary>
    
      <category term="前端" scheme="http://www.chenhanpeng.com/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
      <category term="Vue-CLI3" scheme="http://www.chenhanpeng.com/tags/Vue-CLI3/"/>
    
      <category term="模式" scheme="http://www.chenhanpeng.com/tags/%E6%A8%A1%E5%BC%8F/"/>
    
      <category term="环境变量" scheme="http://www.chenhanpeng.com/tags/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>中文分词Python库介绍</title>
    <link href="http://www.chenhanpeng.com/2019/04/21/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8DPython%E5%BA%93%E4%BB%8B%E7%BB%8D/"/>
    <id>http://www.chenhanpeng.com/2019/04/21/中文分词Python库介绍/</id>
    <published>2019-04-21T09:05:40.000Z</published>
    <updated>2019-04-27T11:56:12.005Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的文章《中文分词》一文中，我们简单介绍了中文分词及其常用的分词方法，本文将介绍几个比较有代表性的支持中文分词的python库。<br>本文所有实例均基于python3.6环境运行。</p><h1 id="jieba"><a href="#jieba" class="headerlink" title="jieba"></a>jieba</h1><p><strong>结巴分词</strong>：使用较为广泛的一款python分词工具，专用于分词的python库，分词效果较好。</p><p><strong>GitHub</strong>： <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">https://github.com/fxsjy/jieba</a></p><h2 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h2><ul><li><p>支持三种分词模式：<br>1、精确模式：试图将句子最精确地切开，适合文本分析<br>2、全模式：把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义<br>3、搜索引擎模式：在精确模式下，对长词再次切分，提高召回率，适合用于搜索引擎分词</p></li><li><p>支持繁体分词</p></li><li>支持自定义词典</li></ul><h2 id="算法："><a href="#算法：" class="headerlink" title="算法："></a>算法：</h2><ul><li>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)</li><li>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合</li><li>对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法</li></ul><h2 id="用法实例："><a href="#用法实例：" class="headerlink" title="用法实例："></a>用法实例：</h2><p>下面我们通过简单的代码来看一下精确模式、全模式、搜索引擎模式三种分词模式的分词效果。代码如下：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding: utf-8 -*-</span></span><br><span class="line">import jieba</span><br><span class="line"><span class="keyword">string</span> = <span class="string">'话说天下大势，分久必合，合久必分。周末七国分争，并入于秦。及秦灭之后，楚、汉分争，又并入于汉。汉朝自高祖斩白蛇而起义，一统天下，后来光武中兴，传至献帝，遂分为三国。推其致乱之由，殆始于桓、灵二帝。桓帝禁锢善类，崇信宦官。及桓帝崩，灵帝即位，大将军窦武、太傅陈蕃共相辅佐。时有宦官曹节等弄权，窦武、陈蕃谋诛之，机事不密，反为所害，中涓自此愈横。'</span></span><br><span class="line"><span class="meta"># 精确模式</span></span><br><span class="line">default_result = jieba.cut(<span class="keyword">string</span>)</span><br><span class="line">print(<span class="string">'精确模式：'</span> + <span class="string">'/'</span>.join(default_result))</span><br><span class="line"></span><br><span class="line"><span class="meta"># 全模式</span></span><br><span class="line">full_result = jieba.cut(<span class="keyword">string</span>, cut_all=True)</span><br><span class="line">print(<span class="string">'全模式：'</span> + <span class="string">'/'</span>.join(full_result))</span><br><span class="line"></span><br><span class="line"><span class="meta"># 搜索引擎模式</span></span><br><span class="line">search_result = jieba.cut_for_search(<span class="keyword">string</span>)</span><br><span class="line">print(<span class="string">'搜索引擎模式：'</span> + <span class="string">'/'</span>.join(search_result))</span><br></pre></td></tr></table></figure></p><p>结果：<br><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">精确模式：话<span class="regexp">/说/</span>天下<span class="regexp">/大势/</span>，<span class="regexp">/分久必合/</span>，<span class="regexp">/合久必分/</span>。<span class="regexp">/周末/</span>七<span class="regexp">/国/</span>分争<span class="regexp">/，/</span>并入<span class="regexp">/于/</span>秦<span class="regexp">/。/</span>及<span class="regexp">/秦灭/</span>之后<span class="regexp">/，/</span>楚<span class="regexp">/、/</span>汉<span class="regexp">/分争/</span>，<span class="regexp">/又/</span>并入<span class="regexp">/于汉/</span>。<span class="regexp">/汉朝/</span>自<span class="regexp">/高祖/</span>斩<span class="regexp">/白蛇/</span>而<span class="regexp">/起义/</span>，<span class="regexp">/一统天下/</span>，<span class="regexp">/后来/</span>光武<span class="regexp">/中兴/</span>，<span class="regexp">/传至/</span>献帝<span class="regexp">/，/</span>遂<span class="regexp">/分为/</span>三国<span class="regexp">/。/</span>推其致<span class="regexp">/乱/</span>之<span class="regexp">/由/</span>，<span class="regexp">/殆/</span>始于<span class="regexp">/桓/</span>、<span class="regexp">/灵/</span>二帝<span class="regexp">/。/</span>桓帝<span class="regexp">/禁锢/</span>善类<span class="regexp">/，/</span>崇信<span class="regexp">/宦官/</span>。<span class="regexp">/及桓帝/</span>崩<span class="regexp">/，/</span>灵帝<span class="regexp">/即位/</span>，<span class="regexp">/大将军/</span>窦武<span class="regexp">/、/</span>太傅陈<span class="regexp">/蕃/</span>共<span class="regexp">/相/</span>辅佐<span class="regexp">/。/</span>时有<span class="regexp">/宦官/</span>曹节<span class="regexp">/等/</span>弄权<span class="regexp">/，/</span>窦武<span class="regexp">/、/</span>陈蕃<span class="regexp">/谋/</span>诛<span class="regexp">/之/</span>，<span class="regexp">/机事不密/</span>，<span class="regexp">/反为/</span>所害<span class="regexp">/，/</span>中<span class="regexp">/涓/</span>自此<span class="regexp">/愈横/</span>。</span><br><span class="line"></span><br><span class="line">全模式：话<span class="regexp">/说/</span>天下<span class="regexp">/大势/</span><span class="regexp">//分久必合//</span><span class="regexp">/合久必分/</span><span class="regexp">//周末/七国/国分/分争//</span><span class="regexp">/并入/</span>于<span class="regexp">/秦/</span><span class="regexp">//及/秦/灭/之后//</span><span class="regexp">/楚/</span><span class="regexp">//汉/分争//</span><span class="regexp">/又/</span>并入<span class="regexp">/于/</span>汉<span class="regexp">///汉朝/自/高祖/斩/白蛇/而/起义//</span><span class="regexp">/一统/</span>一统天下<span class="regexp">/天下/</span><span class="regexp">//后来/光/武/中兴//</span><span class="regexp">/传/</span>至<span class="regexp">/献帝/</span><span class="regexp">//遂/分为/三国//</span><span class="regexp">/推/</span>其<span class="regexp">/致/</span>乱<span class="regexp">/之/</span>由<span class="regexp">///殆/始于/桓//</span><span class="regexp">/灵/</span>二帝<span class="regexp">///桓/帝/禁锢/善类//</span><span class="regexp">/崇信/</span>宦官<span class="regexp">///及/桓/帝/崩//</span><span class="regexp">/灵/</span>帝<span class="regexp">/即位/</span><span class="regexp">//大将/大将军/将军/窦/武//</span><span class="regexp">/太傅/</span>太傅陈<span class="regexp">/蕃/</span>共相<span class="regexp">/相辅/</span>辅佐<span class="regexp">///时/有/宦官/曹/节/等/弄权//</span><span class="regexp">/窦/</span>武<span class="regexp">///陈/蕃/谋/诛/之//</span><span class="regexp">/机事不密/</span><span class="regexp">//反为/所/害//</span><span class="regexp">/中/</span>涓<span class="regexp">/自此/</span>愈<span class="regexp">/横/</span>/</span><br><span class="line"></span><br><span class="line">搜索引擎模式：话<span class="regexp">/说/</span>天下<span class="regexp">/大势/</span>，<span class="regexp">/分久必合/</span>，<span class="regexp">/合久必分/</span>。<span class="regexp">/周末/</span>七<span class="regexp">/国/</span>分争<span class="regexp">/，/</span>并入<span class="regexp">/于/</span>秦<span class="regexp">/。/</span>及<span class="regexp">/秦灭/</span>之后<span class="regexp">/，/</span>楚<span class="regexp">/、/</span>汉<span class="regexp">/分争/</span>，<span class="regexp">/又/</span>并入<span class="regexp">/于汉/</span>。<span class="regexp">/汉朝/</span>自<span class="regexp">/高祖/</span>斩<span class="regexp">/白蛇/</span>而<span class="regexp">/起义/</span>，<span class="regexp">/一统/</span>天下<span class="regexp">/一统天下/</span>，<span class="regexp">/后来/</span>光武<span class="regexp">/中兴/</span>，<span class="regexp">/传至/</span>献帝<span class="regexp">/，/</span>遂<span class="regexp">/分为/</span>三国<span class="regexp">/。/</span>推其致<span class="regexp">/乱/</span>之<span class="regexp">/由/</span>，<span class="regexp">/殆/</span>始于<span class="regexp">/桓/</span>、<span class="regexp">/灵/</span>二帝<span class="regexp">/。/</span>桓帝<span class="regexp">/禁锢/</span>善类<span class="regexp">/，/</span>崇信<span class="regexp">/宦官/</span>。<span class="regexp">/及桓帝/</span>崩<span class="regexp">/，/</span>灵帝<span class="regexp">/即位/</span>，<span class="regexp">/大将/</span>将军<span class="regexp">/大将军/</span>窦武<span class="regexp">/、/</span>太傅<span class="regexp">/太傅陈/</span>蕃<span class="regexp">/共/</span>相<span class="regexp">/辅佐/</span>。<span class="regexp">/时有/</span>宦官<span class="regexp">/曹节/</span>等<span class="regexp">/弄权/</span>，<span class="regexp">/窦武/</span>、<span class="regexp">/陈蕃/</span>谋<span class="regexp">/诛/</span>之<span class="regexp">/，/</span>机事不密<span class="regexp">/，/</span>反为<span class="regexp">/所害/</span>，<span class="regexp">/中/</span>涓<span class="regexp">/自此/</span>愈横/。</span><br></pre></td></tr></table></figure></p><p>从上面的结果来看，分词效果还不错。</p><p>jieba还提供了下面几个功能，具体实现代码可以自行到github上查看，这里不再详细介绍。</p><ul><li>添加自定义词典：开发者还可以指定自己自定义的词典，以便包含jieba词库中没有的词。</li><li>关键词提取：<ul><li>基于TF-IDF算法的关键词抽取(jieba.analyse)</li><li>基于TextRank算法的关键词抽取(jieba.analyse.textrank)</li></ul></li></ul><hr><h1 id="THULAC"><a href="#THULAC" class="headerlink" title="THULAC"></a>THULAC</h1><p><strong>THULAC（THU Lexical Analyzer for Chinese）</strong>由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。</p><p><strong>GitHub</strong>：<a href="https://github.com/thunlp/THULAC-Python" target="_blank" rel="noopener">https://github.com/thunlp/THULAC-Python</a></p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>THULAC具有如下几个特点：</p><ul><li>能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。</li><li>准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。</li><li>速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。</li></ul><h2 id="用法实例"><a href="#用法实例" class="headerlink" title="用法实例"></a>用法实例</h2><p>下面我们开看下THULAC的分词效果，代码如下：<br><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding: utf-8 -*-</span></span><br><span class="line">import thulac</span><br><span class="line"></span><br><span class="line"><span class="meta"># 默认模式，有词性标注</span></span><br><span class="line">thu1 = thulac.thulac()</span><br><span class="line">thu1_result = thu1.cut(<span class="built_in">string</span>, <span class="keyword">text</span>=<span class="literal">True</span>)</span><br><span class="line">print(<span class="comment">'thu1_result：' + thu1_result)</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 只分词，不进行词性标注</span></span><br><span class="line">thu2 = thulac.thulac(seg_only=<span class="literal">True</span>)</span><br><span class="line">thu2_result = thu2.cut(<span class="built_in">string</span>, <span class="keyword">text</span>=<span class="literal">True</span>)</span><br><span class="line">print(<span class="comment">'thu2_result：' + thu2_result)</span></span><br></pre></td></tr></table></figure></p><p>分词结果：<br><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">thu1_result：话<span class="variable">_n</span> 说<span class="variable">_v</span> 天下<span class="variable">_n</span> 大势<span class="variable">_n</span> ，<span class="variable">_w</span> 分久必合<span class="variable">_id</span> ，<span class="variable">_w</span> 合久必分<span class="variable">_id</span> 。<span class="variable">_w</span> 周末<span class="variable">_t</span> 七国分争<span class="variable">_id</span> ，<span class="variable">_w</span> 并入<span class="variable">_v</span> 于<span class="variable">_p</span> 秦<span class="variable">_g</span> 。<span class="variable">_w</span> 及<span class="variable">_c</span> 秦灭<span class="variable">_v</span> 之后<span class="variable">_f</span> ，<span class="variable">_w</span> 楚<span class="variable">_j</span> 、<span class="variable">_w</span> 汉<span class="variable">_g</span> 分争<span class="variable">_v</span> ，<span class="variable">_w</span> 又<span class="variable">_c</span> 并入<span class="variable">_v</span> 于<span class="variable">_p</span> 汉<span class="variable">_g</span> 。<span class="variable">_w</span> 汉朝<span class="variable">_t</span> 自<span class="variable">_r</span> 高<span class="variable">_a</span> 祖斩<span class="variable">_n</span> 白蛇<span class="variable">_n</span> 而<span class="variable">_c</span> 起义<span class="variable">_v</span> ，<span class="variable">_w</span> 一统天下<span class="variable">_id</span> ，<span class="variable">_w</span> 后<span class="variable">_f</span> 来<span class="variable">_v</span> 光武<span class="variable">_ns</span> 中兴<span class="variable">_nz</span> ，<span class="variable">_w</span> 传<span class="variable">_v</span> 至<span class="variable">_d</span> 献<span class="variable">_v</span> 帝<span class="variable">_g</span> ，<span class="variable">_w</span> 遂<span class="variable">_d</span> 分为<span class="variable">_v</span> 三<span class="variable">_m</span> 国<span class="variable">_n</span> 。<span class="variable">_w</span> 推<span class="variable">_v</span> 其<span class="variable">_r</span> 致乱<span class="variable">_v</span> 之<span class="variable">_u</span> 由<span class="variable">_g</span> ，<span class="variable">_w</span> 殆始<span class="variable">_v</span> 于<span class="variable">_p</span> 桓<span class="variable">_g</span> 、<span class="variable">_w</span> 灵二帝<span class="variable">_id</span> 。<span class="variable">_w</span> 桓帝<span class="variable">_np</span> 禁锢<span class="variable">_v</span> 善类<span class="variable">_n</span> ，<span class="variable">_w</span> 崇信<span class="variable">_v</span> 宦官<span class="variable">_n</span> 。<span class="variable">_w</span> 及<span class="variable">_c</span> 桓帝崩<span class="variable">_n</span> ，<span class="variable">_w</span> 灵帝<span class="variable">_n</span> 即位<span class="variable">_n</span> ，<span class="variable">_w</span> 大将军<span class="variable">_n</span> 窦武<span class="variable">_v</span> 、<span class="variable">_w</span> 太傅<span class="variable">_n</span> 陈蕃<span class="variable">_np</span> 共<span class="variable">_d</span> 相<span class="variable">_d</span> 辅佐<span class="variable">_v</span> 。<span class="variable">_w</span> 时<span class="variable">_g</span> 有<span class="variable">_v</span> 宦官<span class="variable">_n</span> 曹节<span class="variable">_np</span> 等<span class="variable">_u</span> 弄权<span class="variable">_n</span> ，<span class="variable">_w</span> 窦武<span class="variable">_np</span> 、<span class="variable">_w</span> 陈蕃<span class="variable">_np</span> 谋诛<span class="variable">_v</span> 之<span class="variable">_u</span> ，<span class="variable">_w</span> 机事不密<span class="variable">_i</span> ，<span class="variable">_w</span> 反<span class="variable">_d</span> 为<span class="variable">_v</span> 所<span class="variable">_u</span> 害<span class="variable">_v</span> ，<span class="variable">_w</span> 中<span class="variable">_j</span> 涓<span class="variable">_j</span> 自此<span class="variable">_d</span> 愈<span class="variable">_d</span> 横<span class="variable">_a</span> 。<span class="variable">_w</span></span><br><span class="line"></span><br><span class="line">thu2_result：话 说 天下 大势 ， 分久必合 ， 合久必分 。 周末 七国分争 ， 并入 于 秦 。 及 秦灭 之后 ， 楚 、 汉分 争 ， 又 并入 于汉 。 汉朝 自 高祖斩白蛇而起义 ， 一统天下 ， 后 来 光武 中兴 ， 传 至 献帝 ， 遂 分为 三 国 。 推 其 致乱 之 由 ， 殆始 于 桓 、 灵二帝 。 桓帝 禁锢善类 ， 崇信 宦官 。 及 桓帝崩 ， 灵帝 即位 ， 大将 军窦武 、 太 傅陈蕃 共 相辅佐 。 时 有 宦官 曹节 等 弄权 ， 窦武 、 陈蕃 谋诛之 ， 机事不密 ， 反 为 所 害 ， 中涓 自此 愈横 。</span><br></pre></td></tr></table></figure></p><hr><h1 id="PKUSeg"><a href="#PKUSeg" class="headerlink" title="PKUSeg"></a>PKUSeg</h1><p><strong>PKUSeg</strong>是由北京大学语言计算与机器学习研究组研制推出的一套全新的中文分词工具包。它简单易用，支持多领域分词，在不同领域的数据上都大幅提高了分词的准确率。</p><p><strong>GitHub</strong>: <a href="https://github.com/lancopku/pkuseg-python" target="_blank" rel="noopener">https://github.com/lancopku/pkuseg-python</a></p><h2 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h2><p>pkuseg有如下几个特点：</p><ul><li>多领域分词</li><li>更高的分词准确率</li><li>支持用户自训练模型</li><li>支持词性标注</li></ul><h2 id="用法实例-1"><a href="#用法实例-1" class="headerlink" title="用法实例"></a>用法实例</h2><p>分词代码：<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pkuseg</span><br><span class="line"># 使用默认配置进行分词</span><br><span class="line">seg = pkuseg.pkuseg()</span><br><span class="line"><span class="built_in">text</span> = seg.cut(<span class="keyword">string</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">text</span>)</span><br></pre></td></tr></table></figure></p><p>分词结果：<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="symbol">'话说</span>', <span class="symbol">'天下</span>', <span class="symbol">'大势</span>', <span class="symbol">'，</span>', <span class="symbol">'分久必合</span>', <span class="symbol">'，</span>', <span class="symbol">'合久必分</span>', <span class="symbol">'。</span>', <span class="symbol">'周末</span>', <span class="symbol">'七</span>', <span class="symbol">'国</span>', <span class="symbol">'分争</span>', <span class="symbol">'，</span>', <span class="symbol">'并入</span>', <span class="symbol">'于</span>', <span class="symbol">'秦</span>', <span class="symbol">'。</span>', <span class="symbol">'及</span>', <span class="symbol">'秦灭</span>', <span class="symbol">'之后</span>', <span class="symbol">'，</span>', <span class="symbol">'楚</span>', <span class="symbol">'、</span>', <span class="symbol">'汉分</span>', <span class="symbol">'争</span>', <span class="symbol">'，</span>', <span class="symbol">'又</span>', <span class="symbol">'并入</span>', <span class="symbol">'于汉</span>', <span class="symbol">'。</span>', <span class="symbol">'汉朝</span>', <span class="symbol">'自</span>', <span class="symbol">'高</span>', <span class="symbol">'祖斩</span>', <span class="symbol">'白蛇而起义</span>', <span class="symbol">'，</span>', <span class="symbol">'一统天下</span>', <span class="symbol">'，</span>', <span class="symbol">'后来</span>', <span class="symbol">'光武</span>', <span class="symbol">'中兴</span>', <span class="symbol">'，</span>', <span class="symbol">'传至</span>', <span class="symbol">'献帝</span>', <span class="symbol">'，</span>', <span class="symbol">'遂</span>', <span class="symbol">'分为</span>', <span class="symbol">'三国</span>', <span class="symbol">'。</span>', <span class="symbol">'推</span>', <span class="symbol">'其</span>', <span class="symbol">'致乱</span>', <span class="symbol">'之</span>', <span class="symbol">'由</span>', <span class="symbol">'，</span>', <span class="symbol">'殆始于桓</span>', <span class="symbol">'、</span>', <span class="symbol">'灵二帝</span>', <span class="symbol">'。</span>', <span class="symbol">'桓帝</span>', <span class="symbol">'禁锢</span>', <span class="symbol">'善类</span>', <span class="symbol">'，</span>', <span class="symbol">'崇信</span>', <span class="symbol">'宦官</span>', <span class="symbol">'。</span>', <span class="symbol">'及</span>', <span class="symbol">'桓帝崩</span>', <span class="symbol">'，</span>', <span class="symbol">'灵帝</span>', <span class="symbol">'即位</span>', <span class="symbol">'，</span>', <span class="symbol">'大将军</span>', <span class="symbol">'窦武</span>', <span class="symbol">'、</span>', <span class="symbol">'太傅</span>', <span class="symbol">'陈</span>', <span class="symbol">'蕃共</span>', <span class="symbol">'相</span>', <span class="symbol">'辅佐</span>', <span class="symbol">'。</span>', <span class="symbol">'时有</span>', <span class="symbol">'宦官</span>', <span class="symbol">'曹节</span>', <span class="symbol">'等</span>', <span class="symbol">'弄权</span>', <span class="symbol">'，</span>', <span class="symbol">'窦武</span>', <span class="symbol">'、</span>', <span class="symbol">'陈</span>', <span class="symbol">'蕃谋</span>', <span class="symbol">'诛之</span>', <span class="symbol">'，</span>', <span class="symbol">'机事不密</span>', <span class="symbol">'，</span>', <span class="symbol">'反为所害</span>', <span class="symbol">'，</span>', <span class="symbol">'中</span>', <span class="symbol">'涓</span>', <span class="symbol">'自此</span>', <span class="symbol">'愈</span>', <span class="symbol">'横</span>', <span class="symbol">'。</span>']</span><br></pre></td></tr></table></figure></p><h2 id="三个库比较"><a href="#三个库比较" class="headerlink" title="三个库比较"></a>三个库比较</h2><p>pkuseg在开源时对其进行测评，pkuseg的作者们选择 THULAC、结巴分词等国内代表分词工具包与 pkuseg 做性能比较。他们选择 Linux 作为测试环境，在新闻数据（MSRA）和混合型文本（CTB8）数据上对不同工具包进行了准确率测试。此外，测试使用的是第二届国际汉语分词评测比赛提供的分词评价脚本。评测结果如下：</p><p><img src="/images/articles/2019/zhongwenfenci_python_1.png#pic_center" alt="Alt"></p><p>我们可以看到，最广泛使用的结巴分词准确率最低，清华的THULAC分词准确率也低于PKUSeg。当然，pkuseg 是在这些数据集上训练的，因此它在这些任务上的准确率也会更高一些，也由此在其开源后对于其高准确率曾引起一些质疑和争议，有兴趣的朋友可以自行百度或点击下面链接查看：<br><a href="https://github.com/lancopku/pkuseg-python/issues/9" target="_blank" rel="noopener">https://github.com/lancopku/pkuseg-python/issues/9</a></p><hr><h1 id="更多python中文分词库"><a href="#更多python中文分词库" class="headerlink" title="更多python中文分词库"></a>更多python中文分词库</h1><p>上面都只是简单介绍了jieba、THULAC和pkuseg三个分词python库，更多用法和参数请到相应的github上查阅。</p><p>还有一些中文分词的python库，这里只提供其github地址，就不一一介绍，有兴趣的朋友自行查看研究。</p><p><strong>SnowNLP</strong>    <a href="https://github.com/isnowfy/snownlp" target="_blank" rel="noopener">https://github.com/isnowfy/snownlp</a></p><p><strong>NLPIR</strong>   <a href="https://github.com/NLPIR-team/NLPIR" target="_blank" rel="noopener">https://github.com/NLPIR-team/NLPIR</a></p><p><strong>FoolNLTK</strong>    <a href="https://github.com/rockyzhengwu/FoolNLTK" target="_blank" rel="noopener">https://github.com/rockyzhengwu/FoolNLTK</a></p><p><strong>LTP</strong>  <a href="https://github.com/HIT-SCIR/pyltp" target="_blank" rel="noopener">https://github.com/HIT-SCIR/pyltp</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在前面的文章《中文分词》一文中，我们简单介绍了中文分词及其常用的分词方法，本文将介绍几个比较有代表性的支持中文分词的python库。&lt;br&gt;本文所有实例均基于python3.6环境运行。&lt;/p&gt;
&lt;h1 id=&quot;jieba&quot;&gt;&lt;a href=&quot;#jieba&quot; class=&quot;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.chenhanpeng.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="中文分词" scheme="http://www.chenhanpeng.com/tags/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统介绍</title>
    <link href="http://www.chenhanpeng.com/2019/04/16/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/"/>
    <id>http://www.chenhanpeng.com/2019/04/16/推荐系统介绍/</id>
    <published>2019-04-15T16:29:20.000Z</published>
    <updated>2019-04-27T11:53:43.579Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>随着互联网的快速发展，我们进入一个信息爆炸的时代。互联网的发展，为我们提供了越来越多的服务平台，比如购物平台、视频播放网站、音乐播放器、社交网婚恋网等等，提供的物品种类也越来越多样。如何更好地满足客户的需求，成了企业的难题。</p><p>在这个信息爆炸的时代，无论是消费者还是信息生产者都遇到了很大的挑战：作为消费者，如何从大量信息中找到自己感兴趣的信息是一件非常困难的事情；作为信息生产者，如何让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。</p><h2 id="二、什么是推荐系统"><a href="#二、什么是推荐系统" class="headerlink" title="二、什么是推荐系统"></a>二、什么是推荐系统</h2><p>推荐系统是一项工程技术解决方案，通过利用机器学习等技术，在用户使用产品进行浏览交互的过程中，系统主动用户展示可能会喜欢的物品，从而促进物品的消费，节省用户时间，提升用户体验，做到资源的优化配置。</p><p>要将推荐系统落地到业务上需要大量的工程开发：涉及到日志打点、日志收集、ETL、分布式计算、特征工程、推荐算法建模、数据存储、提供接口服务、UI展示和交互、推荐效果评估等。</p><p>推荐系统的本质是在用户需求不明确的情况下，从海量的信息中为用户寻找其感兴趣的信息的技术手段。</p><p>推荐系统的任务就是联系用户和信息(物品)，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。</p><p>推荐系统很好满足了用户、平台、内容提供商三方的需求。以淘宝为例：用户及在淘宝上购物的买家，平台即淘宝网站，网站上众多的店主就是内容提供方。通过推荐系统可以更好将商品曝光给要购买的用户，提升社会资源的配置效率。</p><h2 id="三、推荐系统应用领域"><a href="#三、推荐系统应用领域" class="headerlink" title="三、推荐系统应用领域"></a>三、推荐系统应用领域</h2><p>推荐系统应用场景正在不断被挖掘和创造，只要存在大量供用户消费的产品，就有推荐系统发挥价值的地方。</p><p>推荐系统主要应用的领域有如下几类：</p><ul><li>电子商务：淘宝、京东、苏宁易购、亚马逊等</li><li>视频：腾讯视频、优酷、抖音等</li><li>音乐：网易云音乐、QQ音乐等</li><li>生活服务类：美团、携程</li><li>资讯类：头条、一点资讯等</li><li>社交类：陌陌、珍爱网等</li></ul><h2 id="四、常用推荐算法"><a href="#四、常用推荐算法" class="headerlink" title="四、常用推荐算法"></a>四、常用推荐算法</h2><p>接下来我们简单介绍一些推荐系统常用的算法：</p><h4 id="1、基于内容的推荐"><a href="#1、基于内容的推荐" class="headerlink" title="1、基于内容的推荐"></a>1、基于内容的推荐</h4><p>前面我们提到过推荐系统通过技术将用户和物品关联起来，物品本身包含很多属性，用户通过与物品的交互产生行为日志，这些日志可以作为衡量用户对物品偏好的标签，通过这些标签为用户做推荐，这就是基于内容的推荐算法。以音乐播放器推荐歌曲为例，歌曲有歌名、演唱者、类型、年代等标签信息，假设某个用户经常听张杰的歌，或者用户收藏的歌单中大部分都是张杰的歌曲，那么我们可以根据这些兴趣特征为用户推荐张杰的歌。</p><h4 id="2、基于协同过滤的推荐"><a href="#2、基于协同过滤的推荐" class="headerlink" title="2、基于协同过滤的推荐"></a>2、基于协同过滤的推荐</h4><p>用户与物品的交互留下了用户的标记，我们可以通过“物以类聚，人以群分”的思想为用户提供个性化推荐。</p><p>“物以类聚”具体来说就是：如果有许多用户对两个物品有相似的偏好，说明这两个物品是“相似”的，通过推荐与用户喜欢过的物品相似的物品的方法为用户提供个性化推荐，这就是基于物品的协同过滤推荐算法。</p><p>“人以群分”简单来说就是：找到与用户兴趣相同的人，将这些兴趣相同的用户浏览过的物品推荐给用户，这就是基于用户的协同过滤推荐算法。</p><h2 id="五、推荐系统的价值"><a href="#五、推荐系统的价值" class="headerlink" title="五、推荐系统的价值"></a>五、推荐系统的价值</h2><p>接下来我们简单介绍一下推荐系统的价值：</p><p>从<strong>用户</strong>角度来说，推荐系统可以让用户快速从海量信息中找到自己感兴趣的物品，节省了用户时间，提升用户的使用体验。<br>从<strong>平台</strong>角度来看：精准的推荐可以提升用户对平台的粘性，让用户喜欢上平台。平台整体营销收入提升，发现用户更多需求，满足其需求销售更多相关服务，获取更多利润。<br>从<strong>内容提供商</strong>角度来看，提升物品被卖出去的概率，提升提供商的销量。</p><p>文章参考来源：<a href="https://mp.weixin.qq.com/s/DofYtvZCe-7RTicLqYtL4A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/DofYtvZCe-7RTicLqYtL4A</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、背景&quot;&gt;&lt;a href=&quot;#一、背景&quot; class=&quot;headerlink&quot; title=&quot;一、背景&quot;&gt;&lt;/a&gt;一、背景&lt;/h2&gt;&lt;p&gt;随着互联网的快速发展，我们进入一个信息爆炸的时代。互联网的发展，为我们提供了越来越多的服务平台，比如购物平台、视频播放网站、
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.chenhanpeng.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.chenhanpeng.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="推荐系统" scheme="http://www.chenhanpeng.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>中文分词</title>
    <link href="http://www.chenhanpeng.com/2019/04/09/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    <id>http://www.chenhanpeng.com/2019/04/09/中文分词/</id>
    <published>2019-04-09T02:52:01.000Z</published>
    <updated>2019-04-27T11:53:28.275Z</updated>
    
    <content type="html"><![CDATA[<p>中文分词(Chinese Word Segmentation)：是指将一个汉字序列切分为一个个单独的词。中文分词是中文自然语言处理的一个最基本的环节。中文分词与英文分词有很大的不同，对英文而言，一个单词就是一个词，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，需要人为切分。</p><p>根据中文分词的特点，可以把中文分词算法分为四大类：</p><ul><li>基于规则的分词方法</li><li>基于统计的分词方法</li><li>基于语义的分词方法</li><li>基于理解的分词方法</li></ul><hr><h2 id="基于规则的分词方法"><a href="#基于规则的分词方法" class="headerlink" title="基于规则的分词方法"></a>基于规则的分词方法</h2><p>该分词方法又称为机械分词方法、基于字典的分词方法。它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配。若在词典中找到某个字符串，则匹配成功。</p><p>该方法有三个要素：分词词典、文本扫描顺序和匹配原则。文本的扫描顺序有正向扫描、逆向扫描和双向扫描。匹配原则主要有最大匹配、最小匹配、逐词匹配和最佳匹配。</p><p><strong>最大匹配法（MM）</strong>：基本思想是：假设自动分词词典中的最长词条所含汉字的个数为 i，则取被处理材料当前字符串序列中的前 i 个字符作为匹配字段，查找分词词典，若词典中有这样一个 i 字词，则匹配成功，匹配字段作为一个词被切分出来；若词典中找不到这样的一个 i 字词，则匹配失败，匹配字段去掉最后一个汉字，剩下的字符作为新的匹配字段，再进行匹配，如此进行下去，直到匹配成功为止。</p><p><strong>逆向最大匹配法（RMM）</strong>：该方法的分词过程与 MM 法相同，不同的是从句子（或文章）末尾开始处理，每次匹配不成功时去掉的是前面的一个汉字。</p><p><strong>逐词遍历法</strong>：把词典中的词按照由长到短递减的顺序逐字搜索整个待处理的材料，一直到把全部的词切分出来为止。不论分词词典多大，被处理的材料多么小，都得把这个分词词典匹配一遍。</p><p><strong>设立切分标志法</strong>：切分标志有自然和非自然之分。自然切分标志是指文章中出现的非文字符号，如标点符号等；非自然标志是利用词缀和不构成词的词（包 括单音词、复音节词以及象声词等）。设立切分标志法首先收集众多的切分标志，分词时先找出切分标志，把句子切分为一些较短的字段，再用 MM、RMM 或其它的方法进行细加工。这种方法并非真正意义上的分词方法，只是自动分词的一种前处理方式而已，它要额外消耗时间扫描切分标志，增加存储空间存放那些非 自然切分标志。</p><p><strong>最佳匹配法（OM）</strong>：此法分为正向的最佳匹配法和逆向的最佳匹配法，其出发点是：在词典中按词频的大小顺序排列词条，以求缩短对分词词典的检索时 间，达到最佳效果，从而降低分词的时间复杂度，加快分词速度。实质上，这种方法也不是一种纯粹意义上的分词方法，它只是一种对分词词典的组织方式。OM 法的分词词典每条词的前面必须有指明长度的数据项，所以其空间复杂度有所增加，对提高分词精度没有影响，分词处理的时间复杂度有所降低。</p><p>基于规则的分词方法的优点是简单，易于实现。但缺点有很多：匹配速度慢；存在交集型和组合型歧义切分问题；词本身没有一个标准的定义，没有统一标准的词集；不同词典产生的歧义也不同；缺乏自学习的智能性。</p><hr><h2 id="基于统计的分词方法"><a href="#基于统计的分词方法" class="headerlink" title="基于统计的分词方法"></a>基于统计的分词方法</h2><p>基于统计的分词方法是在给定大量已经分词的文本情况下，利用统计机器学习模型学习词语切分的规律，从而实现对未知文本的切分。</p><p>该方法的主要思想：词是稳定的组合，在上下文中，相邻的字同时出现的次数越多，越有可能构成一个词。因此字与字相邻出现的概率能较好反映词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。</p><p>该方法应用的主要统计模型有：N元文法模型(N-gram)、隐马尔科夫模型(Hiden Markov Model, HMM)、最大熵模型(ME)、条件随机场模型(Conditional Random Fields, CRF)等。</p><p>在实际的应用中，这种分词方法都需要使用分词词典来进行字符串匹配分词，同时使用统计方法识别一些新词，即将字符串频率统计和字符串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。</p><hr><h2 id="基于语义的分词方法"><a href="#基于语义的分词方法" class="headerlink" title="基于语义的分词方法"></a>基于语义的分词方法</h2><p>语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。</p><p><strong>扩充转移网络法</strong>：该方法以有限状态机概念为基础。有限状态机只能识别正则语言，对有限状态机作的第一次扩充使其具有递归能力，形成递归转移网络 （RTN）。在RTN 中，弧线上的标志不仅可以是终极符（语言中的单词）或非终极符（词类），还可以调用另外的子网络名字分非终极符（如字或字串的成词条件）。这样，计算机在 运行某个子网络时，就可以调用另外的子网络，还可以递归调用。词法扩充转移网络的使用， 使分词处理和语言理解的句法处理阶段交互成为可能，并且有效地解决了汉语分词的歧义。</p><p><strong>矩阵约束法</strong>：其基本思想是：先建立一个语法约束矩阵和一个语义约束矩阵， 其中元素分别表明具有某词性的词和具有另一词性的词相邻是否符合语法规则， 属于某语义类的词和属于另一词义类的词相邻是否符合逻辑，机器在切分时以之约束分词结果。</p><hr><h2 id="基于理解的分词方法"><a href="#基于理解的分词方法" class="headerlink" title="基于理解的分词方法"></a>基于理解的分词方法</h2><p>基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。目前基于理解的分词方法主要有专家系统分词法和神经网络分词法等。</p><p><strong>专家系统分词法</strong>：从专家系统角度把分词的知识（包括常识性分词知识与消除歧义切分的启发性知识即歧义切分规则）从实现分词过程的推理机中独立出来，使知识库的维护与推理机的实现互不干扰，从而使知识库易于维护和管理。它还具有发现交集歧义字段和多义组合歧义字段的能力和一定的自学习功能。</p><p><strong>神经网络分词法</strong>：该方法是模拟人脑并行，分布处理和建立数值计算模型工作的。它将分词知识所分散隐式的方法存入神经网络内部，通过自学习和训练修改内部权值，以达到正确的分词结果，最后给出神经网络自动分词结果，如使用 LSTM、GRU 等神经网络模型等。</p><p><strong>神经网络专家系统集成式分词法</strong>：该方法首先启动神经网络进行分词，当神经网络对新出现的词不能给出准确切分时，激活专家系统进行分析判断，依据知识库进行推理，得出初步分析，并启动学习机制对神经网络进行训练。该方法可以较充分发挥神经网络与专家系统二者优势，进一步提高分词效率。</p><hr><p><strong>参考来源：</strong><br><a href="https://cuiqingcai.com/5844.html" target="_blank" rel="noopener">https://cuiqingcai.com/5844.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;中文分词(Chinese Word Segmentation)：是指将一个汉字序列切分为一个个单独的词。中文分词是中文自然语言处理的一个最基本的环节。中文分词与英文分词有很大的不同，对英文而言，一个单词就是一个词，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，需
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.chenhanpeng.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://www.chenhanpeng.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="中文分词" scheme="http://www.chenhanpeng.com/tags/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>文本挖掘预处理之TF-IDF</title>
    <link href="http://www.chenhanpeng.com/2019/03/25/TF-IDF-1/"/>
    <id>http://www.chenhanpeng.com/2019/03/25/TF-IDF-1/</id>
    <published>2019-03-25T14:10:10.000Z</published>
    <updated>2019-04-27T11:53:19.381Z</updated>
    
    <content type="html"><![CDATA[<p>TF-IDF（Term Frequency-Inverse Document Frequency）即“词频-反文档频率”，主要由TF和IDF两部分组成。TF-IDF是一种用于资讯检索与资讯探勘的常用加权技术，是一种统计方法，用于评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要程度与它在文件中出现的次数成正比，但同时与它在语料库中出现的频率成反比。</p><p><strong>TF——词频：</strong>一个词在文章中出现的次数。</p><p>在计算词频时，需要注意停用词的过滤。什么是停用词：在文章中出现次数最多的“的”、“是”、“在”等最常用词，但对结果毫无帮助，必须过滤的词。</p><p>TF计算有两种方式，具体公式如下：</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-1.png#pic_center" alt="Alt"><br><img src="/images/articles/2019/20190325TF-IDF-2.png#pic_center" alt="Alt"><br></div><p><strong>IDF——反文档频率：</strong>一个词在所有文章中出现的频率。如果包含这个词的文章越少，IDF越大，则说明词具有很好的类别区分能力。计算公式如下：</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-3.png#pic_center" alt="Alt"><br></div><p>将TF和IDF相乘，就得到一个词的TF-IDF值，某个词对文章的重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-4.png#pic_center" alt="Alt"><br></div><h2 id="TF-IDF总结："><a href="#TF-IDF总结：" class="headerlink" title="TF-IDF总结："></a>TF-IDF总结：</h2><p><strong>优点：</strong>简单快速，结果比较符合实际情况。</p><p><strong>缺点：</strong>单纯以“词频”做衡量标准，不够全面，有时重要的词可能出现的次数不多。</p><h2 id="用python实现TF-IDF的计算"><a href="#用python实现TF-IDF的计算" class="headerlink" title="用python实现TF-IDF的计算"></a>用python实现TF-IDF的计算</h2><p>将下图所示的已经分好词的文章作为语料库，计算101it.seg.cln.txt中的TF-IDF。</p><div align="center"><br><img src="/images/articles/2019/20190325TF-IDF-5.png#pic_center" alt="Alt"><br></div><p>具体python代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 要计算TF-IDF的文章路径</span></span><br><span class="line">file_path = <span class="string">'./data/101it.seg.cln.txt'</span></span><br><span class="line"><span class="comment"># 语料库目录路径</span></span><br><span class="line">data_dir_path = <span class="string">'./data'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 获取文章内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_content</span><span class="params">(file)</span>:</span></span><br><span class="line">    content = open(file, <span class="string">'r'</span>, encoding=<span class="string">'UTF-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 计算IDF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_idf</span><span class="params">(dir_path)</span>:</span></span><br><span class="line">    all_word_set = set()</span><br><span class="line">    article_list = []</span><br><span class="line">    article_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fd <span class="keyword">in</span> os.listdir(dir_path):</span><br><span class="line">        article_count += <span class="number">1</span></span><br><span class="line">        file = dir_path + <span class="string">'/'</span> + fd</span><br><span class="line">        content = read_content(file)</span><br><span class="line">        content_set = set()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> content:</span><br><span class="line">            word_tmp = line.strip().split(<span class="string">' '</span>)</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> word_tmp:</span><br><span class="line">                word = word.strip()</span><br><span class="line">                all_word_set.add(word)</span><br><span class="line">                content_set.add(word)</span><br><span class="line">        article_list.append(content_set)</span><br><span class="line"> </span><br><span class="line">    idf_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> all_word_set:</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> article <span class="keyword">in</span> article_list:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> article:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">        idf_dict[word] = math.log(float(article_count)/(float(count) + <span class="number">1.0</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> idf_dict</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 计算TF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_tf</span><span class="params">(file)</span>:</span></span><br><span class="line">    content = read_content(file)</span><br><span class="line">    word_set = set()</span><br><span class="line">    word_dict = &#123;&#125;</span><br><span class="line">    word_count = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 计算词频和文章总词数</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> content:</span><br><span class="line">        word_tmp = line.strip().split(<span class="string">' '</span>)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_tmp:</span><br><span class="line">            word = word.strip()</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_dict:</span><br><span class="line">                word_dict[word] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word_dict[word] += <span class="number">1</span></span><br><span class="line">            word_count += <span class="number">1</span></span><br><span class="line">            word_set.add(word)</span><br><span class="line">    <span class="comment"># 计算TF</span></span><br><span class="line">    <span class="keyword">for</span> tmp <span class="keyword">in</span> word_set:</span><br><span class="line">        word_dict[tmp] = float(word_dict[tmp])/float(word_count)</span><br><span class="line">    <span class="keyword">return</span> word_dict</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    idf_dict = calculate_idf(data_dir_path)</span><br><span class="line">    tf_dict = calculate_tf(file_path)</span><br><span class="line">    tfidf_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> tf_dict:</span><br><span class="line">        tfidf_dict[key] = tf_dict[key] * idf_dict[key]</span><br><span class="line">    print(tfidf_dict)</span><br></pre></td></tr></table></figure><h2 id="TF-IDF应用："><a href="#TF-IDF应用：" class="headerlink" title="TF-IDF应用："></a>TF-IDF应用：</h2><p>TF-IDF有下面几个应用，具体的实现后续文章再给大家介绍：</p><ul><li><p>提取文章的关键词</p></li><li><p>TF-IDF结合余弦相似度找相似文章</p></li><li><p>给文章自动生成摘要</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;TF-IDF（Term Frequency-Inverse Document Frequency）即“词频-反文档频率”，主要由TF和IDF两部分组成。TF-IDF是一种用于资讯检索与资讯探勘的常用加权技术，是一种统计方法，用于评估一个词对于一个文件集或一个语料库中的其中一
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.chenhanpeng.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="TF-IDF" scheme="http://www.chenhanpeng.com/tags/TF-IDF/"/>
    
  </entry>
  
  <entry>
    <title>K-近邻(KNN)算法</title>
    <link href="http://www.chenhanpeng.com/2019/03/21/knn/"/>
    <id>http://www.chenhanpeng.com/2019/03/21/knn/</id>
    <published>2019-03-21T09:04:17.000Z</published>
    <updated>2019-04-27T11:53:09.722Z</updated>
    
    <content type="html"><![CDATA[<p>K-近邻（KNN，K-Nearest Neighbor）算法是一种基本分类与回归方法，在机器学习分类算法中占有相当大的地位，既是最简单的机器学习算法之一，也是基于实例的学习方法中最基本的，又是最好的文本分类算法之一。</p><p>我们本篇文章只讨论分类问题的KNN算法。</p><h2 id="KNN算法概述"><a href="#KNN算法概述" class="headerlink" title="KNN算法概述"></a>KNN算法概述</h2><p>KNN是通过测量不同特征值之间的距离进行分类。</p><p>KNN算法思路：如果一个样本在特征空间中的k各最相似（即特征空间中最近邻）的样本中的大多数属于某一个类别，则该样本也属于该类别。其中k一个是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。</p><p>KNN算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。</p><p>KNN算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的模型。k值的选择、距离度量和分类决策规则是KNN算法的三个基本要素。</p><h2 id="KNN算法工作原理"><a href="#KNN算法工作原理" class="headerlink" title="KNN算法工作原理"></a>KNN算法工作原理</h2><p>KNN算法工作原理可描述为：<br>1、假设有一个带有标签的训练样本集，其中包含每条数据与所属分类的对应关系。</p><p>2、输入没有带标签的新数据后，计算新数据与训练样本集中每条数据的距离。</p><p>3、对求得的所有距离进行升序排序。</p><p>4、选取k个与新数据距离最小的训练数据。</p><p>5、确定k个训练数据所在类别出现的频率。</p><p>6、返回k个训练数据中出现频率最高的类别作为新数据的分类。</p><h3 id="KNN算法距离计算方式"><a href="#KNN算法距离计算方式" class="headerlink" title="KNN算法距离计算方式"></a>KNN算法距离计算方式</h3><p>在KNN中，通过计算对象间距离来作为各个对象之间的相似性指标，这里的距离计算一般采用欧式距离或者是曼哈顿距离，两种距离的计算公式如下图所示：<br><img src="/images/articles/2019/20190321knn-1.png#pic_center" alt="Alt"></p><h3 id="KNN算法特点："><a href="#KNN算法特点：" class="headerlink" title="KNN算法特点："></a>KNN算法特点：</h3><ul><li><p>优点：精度高、对异常值不敏感、无数据输入假定</p></li><li><p>缺点：计算复杂度高、空间复杂度高</p></li><li><p>适用数据范围：数值型和标称型</p></li></ul><h2 id="KNN算法demo实例"><a href="#KNN算法demo实例" class="headerlink" title="KNN算法demo实例"></a>KNN算法demo实例</h2><p>python（python3.6）实现一个KNN算法的简单demo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建样本数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group = np.array([[<span class="number">1.0</span>, <span class="number">1.1</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0</span>, <span class="number">0.1</span>], [<span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 近邻算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span> <span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    diffMat = np.tile(inX,(dataSet.shape[<span class="number">0</span>],<span class="number">1</span>)) - dataSet <span class="comment">#待分类的输入向量与每个训练数据做差</span></span><br><span class="line">    distance = ((diffMat ** <span class="number">2</span>).sum(axis=<span class="number">1</span>)) ** <span class="number">0.5</span> <span class="comment">#欧氏距离</span></span><br><span class="line">    sortDistanceIndices = distance.argsort() <span class="comment">#从小到大的顺序，返回对应索引值</span></span><br><span class="line">    votelabel = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        votelabel.append(labels[sortDistanceIndices[i]])</span><br><span class="line">    Xlabel = Counter(votelabel).most_common(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Xlabel[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># # 创建数据集和 k-近邻算法</span></span><br><span class="line">    group, labels = createDataSet()</span><br><span class="line">    <span class="comment"># 新数据为[0, 0], k=3</span></span><br><span class="line">    label = classify0([<span class="number">0</span>, <span class="number">0</span>], group, labels, <span class="number">3</span>)</span><br><span class="line">    print(label)</span><br></pre></td></tr></table></figure><p>执行上面代码，在控制台打印输出  B，即为数据[0, 0]的分类。</p><h3 id="方法说明"><a href="#方法说明" class="headerlink" title="方法说明"></a>方法说明</h3><p>上面python代码中有几个方法在这里简单说明一下：</p><ul><li><p>Counter(votelabel).most_common(1)：求votelabel中出现次数最多的元素</p></li><li><p>np.tile(A,B)：若B为int型：在列方向上将A重复B次 若B为元组(m,n):将A在列方向上重复n次，在行方向上重复m次</p></li><li><p>sum(axis=1)：函数的axis参数，axis=0:按列相加；axis=1:按行的方向相加，即每行数据求和</p></li><li><p>argsort：将数组的值按从小到大排序后，输出索引值</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;K-近邻（KNN，K-Nearest Neighbor）算法是一种基本分类与回归方法，在机器学习分类算法中占有相当大的地位，既是最简单的机器学习算法之一，也是基于实例的学习方法中最基本的，又是最好的文本分类算法之一。&lt;/p&gt;
&lt;p&gt;我们本篇文章只讨论分类问题的KNN算法。&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.chenhanpeng.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="KNN" scheme="http://www.chenhanpeng.com/tags/KNN/"/>
    
      <category term="机器学习" scheme="http://www.chenhanpeng.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://www.chenhanpeng.com/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
