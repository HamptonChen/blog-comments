{"meta":{"title":"程序员的修行之路","subtitle":"Hampton Chen's Blog","description":null,"author":"Hampton Chen","url":"http://yoursite.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2019-03-20T08:33:13.864Z","updated":"2019-03-20T08:33:13.864Z","comments":false,"path":"/404.html","permalink":"http://yoursite.com//404.html","excerpt":"","text":""},{"title":"archives","date":"2019-03-19T15:35:45.000Z","updated":"2019-03-19T15:35:45.761Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"关于自己","date":"2019-04-09T14:08:03.038Z","updated":"2019-04-09T14:08:03.038Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"程序猿一枚"},{"title":"书单","date":"2019-03-20T08:33:13.869Z","updated":"2019-03-20T08:33:13.869Z","comments":false,"path":"books/index.html","permalink":"http://yoursite.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-03-20T08:33:13.870Z","updated":"2019-03-20T08:33:13.870Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-03-20T08:33:13.872Z","updated":"2019-03-20T08:33:13.872Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-03-20T08:33:13.873Z","updated":"2019-03-20T08:33:13.873Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-03-20T08:33:13.875Z","updated":"2019-03-20T08:33:13.875Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Vue-CLI3 环境变量和模式","slug":"vue-cli3-环境变量和模式","date":"2019-04-29T14:46:30.000Z","updated":"2019-04-29T14:54:29.079Z","comments":true,"path":"2019/04/29/vue-cli3-环境变量和模式/","link":"","permalink":"http://yoursite.com/2019/04/29/vue-cli3-环境变量和模式/","excerpt":"","text":"前段时间工作中用Vue-CLI3构建的Vue工程一些静态资源，比如静态H5页面、图片、图标等等，我们一般放在固定的一些服务器上，链接前缀一般相对固定，但我们打包发布一般要区分测试环境和生产环境，此时的静态资源路径也需要区分测试和生产，如果每次打包都要根据部署的环境去修改路径十分麻烦，这时候vue-cli的模式和环境变量则能够很好地解决这个麻烦。 模式模式是Vue CLI项目中一个重要的概念。默认情况下，一个Vue CLI项目有三个模式： development 模式用于vue-cli-service serve production 模式用于vue-cli-service build和vue-cli-service test:e2e test 模式用于vue-cli-service test:unit 模式不同于NODE_ENV，一个模式可以包含多个环境变量。每个模式都会将NODE_ENV的值设置为模式的名称，比如：development模式下NODE_ENV的值会被设置为development 当然，我们也可以通过为.env文件增加后缀来设置某个模式下特有的环境变量。 示例：test模式我们在项目根目录下创建一个名为.env.test和.env的文件 12// .env文件：VUE_APP_TITLE=VUE-CLI3-DEMO 123// .env.test文件NODE_ENV=productionVUE_APP_TITLE=VUE-CLI3-DEMO(test) vue-cli-service build 会加载可能存在的 .env、.env.production 和 .env.production.local 文件然后构建出生产环境应用； vue-cli-service build –mode test 会在 staging 模式下加载可能存在的 .env、.env.test 和 .env.test.local 文件然后构建出生产环境应用。 这两种情况下，根据 NODE_ENV，构建出的应用都是生产环境应用，但是在 test 版本中，process.env.VUE_APP_TITLE 被覆写成了另一个值。 我们在vue.config.js文件中添加console.log(process.env.VUE_APP_TITLE) 在package.json文件中添加”build-test”: “vue-cli-service build –mode test” 执行npm run build和npm run build-test 通过查看控制台打印出的分别是VUE-CLI3-DEMO和VUE-CLI3-DEMO(test)，由此可见不同模式下的环境变量不同。 环境变量只有以VUE_APP_开头的变量才会被webpack.DefinePlugin静态嵌入到客户端侧的包中，我们可以在代码中以下面的方式访问：process.env.VUE_APP_TITLE 除了VUE_APP_*变量外，还有两个始终可用的特殊变量： NODE_ENV 值为development、productin、test中的一个。 BASE_URL 与vue.config.js中的publicPath相符，即应用部署的基础路径 回到我们最开始的关于静态资源在不同环境下的路径问题，我们可以分别创建.env.test和.env.production两个文件，在文件中添加变量VUE_APP_STATIC_BASE_URL，根据不同环境赋予不同的值，在代码中用到静态资源的时候通过process.env.VUE_APP_STATIC_BASE_URL + 静态资源后续具体路径，再package.json中添加相应的模式打包命令，这样就可以比较好解决我们最开始提出的问题了。","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"},{"name":"Vue","slug":"前端/Vue","permalink":"http://yoursite.com/categories/前端/Vue/"}],"tags":[{"name":"Vue-CLI3","slug":"Vue-CLI3","permalink":"http://yoursite.com/tags/Vue-CLI3/"},{"name":"模式","slug":"模式","permalink":"http://yoursite.com/tags/模式/"},{"name":"环境变量","slug":"环境变量","permalink":"http://yoursite.com/tags/环境变量/"}]},{"title":"中文分词Python库介绍","slug":"中文分词Python库介绍","date":"2019-04-21T09:05:40.000Z","updated":"2019-04-27T11:56:12.005Z","comments":true,"path":"2019/04/21/中文分词Python库介绍/","link":"","permalink":"http://yoursite.com/2019/04/21/中文分词Python库介绍/","excerpt":"","text":"在前面的文章《中文分词》一文中，我们简单介绍了中文分词及其常用的分词方法，本文将介绍几个比较有代表性的支持中文分词的python库。本文所有实例均基于python3.6环境运行。 jieba结巴分词：使用较为广泛的一款python分词工具，专用于分词的python库，分词效果较好。 GitHub： https://github.com/fxsjy/jieba 特点： 支持三种分词模式：1、精确模式：试图将句子最精确地切开，适合文本分析2、全模式：把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义3、搜索引擎模式：在精确模式下，对长词再次切分，提高召回率，适合用于搜索引擎分词 支持繁体分词 支持自定义词典 算法： 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法 用法实例：下面我们通过简单的代码来看一下精确模式、全模式、搜索引擎模式三种分词模式的分词效果。代码如下：1234567891011121314# -*- coding: utf-8 -*-import jiebastring = '话说天下大势，分久必合，合久必分。周末七国分争，并入于秦。及秦灭之后，楚、汉分争，又并入于汉。汉朝自高祖斩白蛇而起义，一统天下，后来光武中兴，传至献帝，遂分为三国。推其致乱之由，殆始于桓、灵二帝。桓帝禁锢善类，崇信宦官。及桓帝崩，灵帝即位，大将军窦武、太傅陈蕃共相辅佐。时有宦官曹节等弄权，窦武、陈蕃谋诛之，机事不密，反为所害，中涓自此愈横。'# 精确模式default_result = jieba.cut(string)print('精确模式：' + '/'.join(default_result))# 全模式full_result = jieba.cut(string, cut_all=True)print('全模式：' + '/'.join(full_result))# 搜索引擎模式search_result = jieba.cut_for_search(string)print('搜索引擎模式：' + '/'.join(search_result)) 结果：12345精确模式：话/说/天下/大势/，/分久必合/，/合久必分/。/周末/七/国/分争/，/并入/于/秦/。/及/秦灭/之后/，/楚/、/汉/分争/，/又/并入/于汉/。/汉朝/自/高祖/斩/白蛇/而/起义/，/一统天下/，/后来/光武/中兴/，/传至/献帝/，/遂/分为/三国/。/推其致/乱/之/由/，/殆/始于/桓/、/灵/二帝/。/桓帝/禁锢/善类/，/崇信/宦官/。/及桓帝/崩/，/灵帝/即位/，/大将军/窦武/、/太傅陈/蕃/共/相/辅佐/。/时有/宦官/曹节/等/弄权/，/窦武/、/陈蕃/谋/诛/之/，/机事不密/，/反为/所害/，/中/涓/自此/愈横/。全模式：话/说/天下/大势///分久必合///合久必分///周末/七国/国分/分争///并入/于/秦///及/秦/灭/之后///楚///汉/分争///又/并入/于/汉///汉朝/自/高祖/斩/白蛇/而/起义///一统/一统天下/天下///后来/光/武/中兴///传/至/献帝///遂/分为/三国///推/其/致/乱/之/由///殆/始于/桓///灵/二帝///桓/帝/禁锢/善类///崇信/宦官///及/桓/帝/崩///灵/帝/即位///大将/大将军/将军/窦/武///太傅/太傅陈/蕃/共相/相辅/辅佐///时/有/宦官/曹/节/等/弄权///窦/武///陈/蕃/谋/诛/之///机事不密///反为/所/害///中/涓/自此/愈/横//搜索引擎模式：话/说/天下/大势/，/分久必合/，/合久必分/。/周末/七/国/分争/，/并入/于/秦/。/及/秦灭/之后/，/楚/、/汉/分争/，/又/并入/于汉/。/汉朝/自/高祖/斩/白蛇/而/起义/，/一统/天下/一统天下/，/后来/光武/中兴/，/传至/献帝/，/遂/分为/三国/。/推其致/乱/之/由/，/殆/始于/桓/、/灵/二帝/。/桓帝/禁锢/善类/，/崇信/宦官/。/及桓帝/崩/，/灵帝/即位/，/大将/将军/大将军/窦武/、/太傅/太傅陈/蕃/共/相/辅佐/。/时有/宦官/曹节/等/弄权/，/窦武/、/陈蕃/谋/诛/之/，/机事不密/，/反为/所害/，/中/涓/自此/愈横/。 从上面的结果来看，分词效果还不错。 jieba还提供了下面几个功能，具体实现代码可以自行到github上查看，这里不再详细介绍。 添加自定义词典：开发者还可以指定自己自定义的词典，以便包含jieba词库中没有的词。 关键词提取： 基于TF-IDF算法的关键词抽取(jieba.analyse) 基于TextRank算法的关键词抽取(jieba.analyse.textrank) THULACTHULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。 GitHub：https://github.com/thunlp/THULAC-Python 特点THULAC具有如下几个特点： 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。 用法实例下面我们开看下THULAC的分词效果，代码如下：123456789101112# -*- coding: utf-8 -*-import thulac# 默认模式，有词性标注thu1 = thulac.thulac()thu1_result = thu1.cut(string, text=True)print('thu1_result：' + thu1_result)# 只分词，不进行词性标注thu2 = thulac.thulac(seg_only=True)thu2_result = thu2.cut(string, text=True)print('thu2_result：' + thu2_result) 分词结果：123thu1_result：话_n 说_v 天下_n 大势_n ，_w 分久必合_id ，_w 合久必分_id 。_w 周末_t 七国分争_id ，_w 并入_v 于_p 秦_g 。_w 及_c 秦灭_v 之后_f ，_w 楚_j 、_w 汉_g 分争_v ，_w 又_c 并入_v 于_p 汉_g 。_w 汉朝_t 自_r 高_a 祖斩_n 白蛇_n 而_c 起义_v ，_w 一统天下_id ，_w 后_f 来_v 光武_ns 中兴_nz ，_w 传_v 至_d 献_v 帝_g ，_w 遂_d 分为_v 三_m 国_n 。_w 推_v 其_r 致乱_v 之_u 由_g ，_w 殆始_v 于_p 桓_g 、_w 灵二帝_id 。_w 桓帝_np 禁锢_v 善类_n ，_w 崇信_v 宦官_n 。_w 及_c 桓帝崩_n ，_w 灵帝_n 即位_n ，_w 大将军_n 窦武_v 、_w 太傅_n 陈蕃_np 共_d 相_d 辅佐_v 。_w 时_g 有_v 宦官_n 曹节_np 等_u 弄权_n ，_w 窦武_np 、_w 陈蕃_np 谋诛_v 之_u ，_w 机事不密_i ，_w 反_d 为_v 所_u 害_v ，_w 中_j 涓_j 自此_d 愈_d 横_a 。_wthu2_result：话 说 天下 大势 ， 分久必合 ， 合久必分 。 周末 七国分争 ， 并入 于 秦 。 及 秦灭 之后 ， 楚 、 汉分 争 ， 又 并入 于汉 。 汉朝 自 高祖斩白蛇而起义 ， 一统天下 ， 后 来 光武 中兴 ， 传 至 献帝 ， 遂 分为 三 国 。 推 其 致乱 之 由 ， 殆始 于 桓 、 灵二帝 。 桓帝 禁锢善类 ， 崇信 宦官 。 及 桓帝崩 ， 灵帝 即位 ， 大将 军窦武 、 太 傅陈蕃 共 相辅佐 。 时 有 宦官 曹节 等 弄权 ， 窦武 、 陈蕃 谋诛之 ， 机事不密 ， 反 为 所 害 ， 中涓 自此 愈横 。 PKUSegPKUSeg是由北京大学语言计算与机器学习研究组研制推出的一套全新的中文分词工具包。它简单易用，支持多领域分词，在不同领域的数据上都大幅提高了分词的准确率。 GitHub: https://github.com/lancopku/pkuseg-python 特点pkuseg有如下几个特点： 多领域分词 更高的分词准确率 支持用户自训练模型 支持词性标注 用法实例分词代码：12345import pkuseg# 使用默认配置进行分词seg = pkuseg.pkuseg()text = seg.cut(string)print(text) 分词结果：1['话说', '天下', '大势', '，', '分久必合', '，', '合久必分', '。', '周末', '七', '国', '分争', '，', '并入', '于', '秦', '。', '及', '秦灭', '之后', '，', '楚', '、', '汉分', '争', '，', '又', '并入', '于汉', '。', '汉朝', '自', '高', '祖斩', '白蛇而起义', '，', '一统天下', '，', '后来', '光武', '中兴', '，', '传至', '献帝', '，', '遂', '分为', '三国', '。', '推', '其', '致乱', '之', '由', '，', '殆始于桓', '、', '灵二帝', '。', '桓帝', '禁锢', '善类', '，', '崇信', '宦官', '。', '及', '桓帝崩', '，', '灵帝', '即位', '，', '大将军', '窦武', '、', '太傅', '陈', '蕃共', '相', '辅佐', '。', '时有', '宦官', '曹节', '等', '弄权', '，', '窦武', '、', '陈', '蕃谋', '诛之', '，', '机事不密', '，', '反为所害', '，', '中', '涓', '自此', '愈', '横', '。'] 三个库比较pkuseg在开源时对其进行测评，pkuseg的作者们选择 THULAC、结巴分词等国内代表分词工具包与 pkuseg 做性能比较。他们选择 Linux 作为测试环境，在新闻数据（MSRA）和混合型文本（CTB8）数据上对不同工具包进行了准确率测试。此外，测试使用的是第二届国际汉语分词评测比赛提供的分词评价脚本。评测结果如下： 我们可以看到，最广泛使用的结巴分词准确率最低，清华的THULAC分词准确率也低于PKUSeg。当然，pkuseg 是在这些数据集上训练的，因此它在这些任务上的准确率也会更高一些，也由此在其开源后对于其高准确率曾引起一些质疑和争议，有兴趣的朋友可以自行百度或点击下面链接查看：https://github.com/lancopku/pkuseg-python/issues/9 更多python中文分词库上面都只是简单介绍了jieba、THULAC和pkuseg三个分词python库，更多用法和参数请到相应的github上查阅。 还有一些中文分词的python库，这里只提供其github地址，就不一一介绍，有兴趣的朋友自行查看研究。 SnowNLP https://github.com/isnowfy/snownlp NLPIR https://github.com/NLPIR-team/NLPIR FoolNLTK https://github.com/rockyzhengwu/FoolNLTK LTP https://github.com/HIT-SCIR/pyltp","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"中文分词","slug":"中文分词","permalink":"http://yoursite.com/tags/中文分词/"}]},{"title":"推荐系统介绍","slug":"推荐系统介绍","date":"2019-04-15T16:29:20.000Z","updated":"2019-04-27T11:53:43.579Z","comments":true,"path":"2019/04/16/推荐系统介绍/","link":"","permalink":"http://yoursite.com/2019/04/16/推荐系统介绍/","excerpt":"","text":"一、背景随着互联网的快速发展，我们进入一个信息爆炸的时代。互联网的发展，为我们提供了越来越多的服务平台，比如购物平台、视频播放网站、音乐播放器、社交网婚恋网等等，提供的物品种类也越来越多样。如何更好地满足客户的需求，成了企业的难题。 在这个信息爆炸的时代，无论是消费者还是信息生产者都遇到了很大的挑战：作为消费者，如何从大量信息中找到自己感兴趣的信息是一件非常困难的事情；作为信息生产者，如何让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。 二、什么是推荐系统推荐系统是一项工程技术解决方案，通过利用机器学习等技术，在用户使用产品进行浏览交互的过程中，系统主动用户展示可能会喜欢的物品，从而促进物品的消费，节省用户时间，提升用户体验，做到资源的优化配置。 要将推荐系统落地到业务上需要大量的工程开发：涉及到日志打点、日志收集、ETL、分布式计算、特征工程、推荐算法建模、数据存储、提供接口服务、UI展示和交互、推荐效果评估等。 推荐系统的本质是在用户需求不明确的情况下，从海量的信息中为用户寻找其感兴趣的信息的技术手段。 推荐系统的任务就是联系用户和信息(物品)，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。 推荐系统很好满足了用户、平台、内容提供商三方的需求。以淘宝为例：用户及在淘宝上购物的买家，平台即淘宝网站，网站上众多的店主就是内容提供方。通过推荐系统可以更好将商品曝光给要购买的用户，提升社会资源的配置效率。 三、推荐系统应用领域推荐系统应用场景正在不断被挖掘和创造，只要存在大量供用户消费的产品，就有推荐系统发挥价值的地方。 推荐系统主要应用的领域有如下几类： 电子商务：淘宝、京东、苏宁易购、亚马逊等 视频：腾讯视频、优酷、抖音等 音乐：网易云音乐、QQ音乐等 生活服务类：美团、携程 资讯类：头条、一点资讯等 社交类：陌陌、珍爱网等 四、常用推荐算法接下来我们简单介绍一些推荐系统常用的算法： 1、基于内容的推荐前面我们提到过推荐系统通过技术将用户和物品关联起来，物品本身包含很多属性，用户通过与物品的交互产生行为日志，这些日志可以作为衡量用户对物品偏好的标签，通过这些标签为用户做推荐，这就是基于内容的推荐算法。以音乐播放器推荐歌曲为例，歌曲有歌名、演唱者、类型、年代等标签信息，假设某个用户经常听张杰的歌，或者用户收藏的歌单中大部分都是张杰的歌曲，那么我们可以根据这些兴趣特征为用户推荐张杰的歌。 2、基于协同过滤的推荐用户与物品的交互留下了用户的标记，我们可以通过“物以类聚，人以群分”的思想为用户提供个性化推荐。 “物以类聚”具体来说就是：如果有许多用户对两个物品有相似的偏好，说明这两个物品是“相似”的，通过推荐与用户喜欢过的物品相似的物品的方法为用户提供个性化推荐，这就是基于物品的协同过滤推荐算法。 “人以群分”简单来说就是：找到与用户兴趣相同的人，将这些兴趣相同的用户浏览过的物品推荐给用户，这就是基于用户的协同过滤推荐算法。 五、推荐系统的价值接下来我们简单介绍一下推荐系统的价值： 从用户角度来说，推荐系统可以让用户快速从海量信息中找到自己感兴趣的物品，节省了用户时间，提升用户的使用体验。从平台角度来看：精准的推荐可以提升用户对平台的粘性，让用户喜欢上平台。平台整体营销收入提升，发现用户更多需求，满足其需求销售更多相关服务，获取更多利润。从内容提供商角度来看，提升物品被卖出去的概率，提升提供商的销量。 文章参考来源：https://mp.weixin.qq.com/s/DofYtvZCe-7RTicLqYtL4A","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/大数据/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://yoursite.com/tags/推荐系统/"},{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"}]},{"title":"中文分词","slug":"中文分词","date":"2019-04-09T02:52:01.000Z","updated":"2019-04-27T11:53:28.275Z","comments":true,"path":"2019/04/09/中文分词/","link":"","permalink":"http://yoursite.com/2019/04/09/中文分词/","excerpt":"","text":"中文分词(Chinese Word Segmentation)：是指将一个汉字序列切分为一个个单独的词。中文分词是中文自然语言处理的一个最基本的环节。中文分词与英文分词有很大的不同，对英文而言，一个单词就是一个词，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，需要人为切分。 根据中文分词的特点，可以把中文分词算法分为四大类： 基于规则的分词方法 基于统计的分词方法 基于语义的分词方法 基于理解的分词方法 基于规则的分词方法该分词方法又称为机械分词方法、基于字典的分词方法。它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配。若在词典中找到某个字符串，则匹配成功。 该方法有三个要素：分词词典、文本扫描顺序和匹配原则。文本的扫描顺序有正向扫描、逆向扫描和双向扫描。匹配原则主要有最大匹配、最小匹配、逐词匹配和最佳匹配。 最大匹配法（MM）：基本思想是：假设自动分词词典中的最长词条所含汉字的个数为 i，则取被处理材料当前字符串序列中的前 i 个字符作为匹配字段，查找分词词典，若词典中有这样一个 i 字词，则匹配成功，匹配字段作为一个词被切分出来；若词典中找不到这样的一个 i 字词，则匹配失败，匹配字段去掉最后一个汉字，剩下的字符作为新的匹配字段，再进行匹配，如此进行下去，直到匹配成功为止。 逆向最大匹配法（RMM）：该方法的分词过程与 MM 法相同，不同的是从句子（或文章）末尾开始处理，每次匹配不成功时去掉的是前面的一个汉字。 逐词遍历法：把词典中的词按照由长到短递减的顺序逐字搜索整个待处理的材料，一直到把全部的词切分出来为止。不论分词词典多大，被处理的材料多么小，都得把这个分词词典匹配一遍。 设立切分标志法：切分标志有自然和非自然之分。自然切分标志是指文章中出现的非文字符号，如标点符号等；非自然标志是利用词缀和不构成词的词（包 括单音词、复音节词以及象声词等）。设立切分标志法首先收集众多的切分标志，分词时先找出切分标志，把句子切分为一些较短的字段，再用 MM、RMM 或其它的方法进行细加工。这种方法并非真正意义上的分词方法，只是自动分词的一种前处理方式而已，它要额外消耗时间扫描切分标志，增加存储空间存放那些非 自然切分标志。 最佳匹配法（OM）：此法分为正向的最佳匹配法和逆向的最佳匹配法，其出发点是：在词典中按词频的大小顺序排列词条，以求缩短对分词词典的检索时 间，达到最佳效果，从而降低分词的时间复杂度，加快分词速度。实质上，这种方法也不是一种纯粹意义上的分词方法，它只是一种对分词词典的组织方式。OM 法的分词词典每条词的前面必须有指明长度的数据项，所以其空间复杂度有所增加，对提高分词精度没有影响，分词处理的时间复杂度有所降低。 基于规则的分词方法的优点是简单，易于实现。但缺点有很多：匹配速度慢；存在交集型和组合型歧义切分问题；词本身没有一个标准的定义，没有统一标准的词集；不同词典产生的歧义也不同；缺乏自学习的智能性。 基于统计的分词方法基于统计的分词方法是在给定大量已经分词的文本情况下，利用统计机器学习模型学习词语切分的规律，从而实现对未知文本的切分。 该方法的主要思想：词是稳定的组合，在上下文中，相邻的字同时出现的次数越多，越有可能构成一个词。因此字与字相邻出现的概率能较好反映词的可信度。可以对训练文本中相邻出现的各个字的组合的频度进行统计，计算它们之间的互现信息。互现信息体现了汉字之间结合关系的紧密程度。当紧密程 度高于某一个阈值时，便可以认为此字组可能构成了一个词。该方法又称为无字典分词。 该方法应用的主要统计模型有：N元文法模型(N-gram)、隐马尔科夫模型(Hiden Markov Model, HMM)、最大熵模型(ME)、条件随机场模型(Conditional Random Fields, CRF)等。 在实际的应用中，这种分词方法都需要使用分词词典来进行字符串匹配分词，同时使用统计方法识别一些新词，即将字符串频率统计和字符串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。 基于语义的分词方法语义分词法引入了语义分析，对自然语言自身的语言信息进行更多的处理，如扩充转移网络法、知识分词语义分析法、邻接约束法、综合匹配法、后缀分词法、特征词库法、矩阵约束法、语法分析法等。 扩充转移网络法：该方法以有限状态机概念为基础。有限状态机只能识别正则语言，对有限状态机作的第一次扩充使其具有递归能力，形成递归转移网络 （RTN）。在RTN 中，弧线上的标志不仅可以是终极符（语言中的单词）或非终极符（词类），还可以调用另外的子网络名字分非终极符（如字或字串的成词条件）。这样，计算机在 运行某个子网络时，就可以调用另外的子网络，还可以递归调用。词法扩充转移网络的使用， 使分词处理和语言理解的句法处理阶段交互成为可能，并且有效地解决了汉语分词的歧义。 矩阵约束法：其基本思想是：先建立一个语法约束矩阵和一个语义约束矩阵， 其中元素分别表明具有某词性的词和具有另一词性的词相邻是否符合语法规则， 属于某语义类的词和属于另一词义类的词相邻是否符合逻辑，机器在切分时以之约束分词结果。 基于理解的分词方法基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。目前基于理解的分词方法主要有专家系统分词法和神经网络分词法等。 专家系统分词法：从专家系统角度把分词的知识（包括常识性分词知识与消除歧义切分的启发性知识即歧义切分规则）从实现分词过程的推理机中独立出来，使知识库的维护与推理机的实现互不干扰，从而使知识库易于维护和管理。它还具有发现交集歧义字段和多义组合歧义字段的能力和一定的自学习功能。 神经网络分词法：该方法是模拟人脑并行，分布处理和建立数值计算模型工作的。它将分词知识所分散隐式的方法存入神经网络内部，通过自学习和训练修改内部权值，以达到正确的分词结果，最后给出神经网络自动分词结果，如使用 LSTM、GRU 等神经网络模型等。 神经网络专家系统集成式分词法：该方法首先启动神经网络进行分词，当神经网络对新出现的词不能给出准确切分时，激活专家系统进行分析判断，依据知识库进行推理，得出初步分析，并启动学习机制对神经网络进行训练。该方法可以较充分发挥神经网络与专家系统二者优势，进一步提高分词效率。 参考来源：https://cuiqingcai.com/5844.html","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"中文分词","slug":"中文分词","permalink":"http://yoursite.com/tags/中文分词/"}]},{"title":"文本挖掘预处理之TF-IDF","slug":"TF-IDF-1","date":"2019-03-25T14:10:10.000Z","updated":"2019-04-27T11:53:19.381Z","comments":true,"path":"2019/03/25/TF-IDF-1/","link":"","permalink":"http://yoursite.com/2019/03/25/TF-IDF-1/","excerpt":"","text":"TF-IDF（Term Frequency-Inverse Document Frequency）即“词频-反文档频率”，主要由TF和IDF两部分组成。TF-IDF是一种用于资讯检索与资讯探勘的常用加权技术，是一种统计方法，用于评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要程度与它在文件中出现的次数成正比，但同时与它在语料库中出现的频率成反比。 TF——词频：一个词在文章中出现的次数。 在计算词频时，需要注意停用词的过滤。什么是停用词：在文章中出现次数最多的“的”、“是”、“在”等最常用词，但对结果毫无帮助，必须过滤的词。 TF计算有两种方式，具体公式如下： IDF——反文档频率：一个词在所有文章中出现的频率。如果包含这个词的文章越少，IDF越大，则说明词具有很好的类别区分能力。计算公式如下： 将TF和IDF相乘，就得到一个词的TF-IDF值，某个词对文章的重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。 TF-IDF总结：优点：简单快速，结果比较符合实际情况。 缺点：单纯以“词频”做衡量标准，不够全面，有时重要的词可能出现的次数不多。 用python实现TF-IDF的计算将下图所示的已经分好词的文章作为语料库，计算101it.seg.cln.txt中的TF-IDF。 具体python代码实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# -*- coding: utf-8 -*-import osimport math # 要计算TF-IDF的文章路径file_path = './data/101it.seg.cln.txt'# 语料库目录路径data_dir_path = './data' # 获取文章内容def read_content(file): content = open(file, 'r', encoding='UTF-8') return content # 计算IDFdef calculate_idf(dir_path): all_word_set = set() article_list = [] article_count = 0 for fd in os.listdir(dir_path): article_count += 1 file = dir_path + '/' + fd content = read_content(file) content_set = set() for line in content: word_tmp = line.strip().split(' ') for word in word_tmp: word = word.strip() all_word_set.add(word) content_set.add(word) article_list.append(content_set) idf_dict = &#123;&#125; for word in all_word_set: count = 0 for article in article_list: if word in article: count += 1 idf_dict[word] = math.log(float(article_count)/(float(count) + 1.0)) return idf_dict # 计算TFdef calculate_tf(file): content = read_content(file) word_set = set() word_dict = &#123;&#125; word_count = 0 # 计算词频和文章总词数 for line in content: word_tmp = line.strip().split(' ') for word in word_tmp: word = word.strip() if word not in word_dict: word_dict[word] = 1 else: word_dict[word] += 1 word_count += 1 word_set.add(word) # 计算TF for tmp in word_set: word_dict[tmp] = float(word_dict[tmp])/float(word_count) return word_dict if __name__ == \"__main__\": idf_dict = calculate_idf(data_dir_path) tf_dict = calculate_tf(file_path) tfidf_dict = &#123;&#125; for key in tf_dict: tfidf_dict[key] = tf_dict[key] * idf_dict[key] print(tfidf_dict) TF-IDF应用：TF-IDF有下面几个应用，具体的实现后续文章再给大家介绍： 提取文章的关键词 TF-IDF结合余弦相似度找相似文章 给文章自动生成摘要","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"TF-IDF","slug":"TF-IDF","permalink":"http://yoursite.com/tags/TF-IDF/"}]},{"title":"K-近邻(KNN)算法","slug":"knn","date":"2019-03-21T09:04:17.000Z","updated":"2019-04-27T11:53:09.722Z","comments":true,"path":"2019/03/21/knn/","link":"","permalink":"http://yoursite.com/2019/03/21/knn/","excerpt":"","text":"K-近邻（KNN，K-Nearest Neighbor）算法是一种基本分类与回归方法，在机器学习分类算法中占有相当大的地位，既是最简单的机器学习算法之一，也是基于实例的学习方法中最基本的，又是最好的文本分类算法之一。 我们本篇文章只讨论分类问题的KNN算法。 KNN算法概述KNN是通过测量不同特征值之间的距离进行分类。 KNN算法思路：如果一个样本在特征空间中的k各最相似（即特征空间中最近邻）的样本中的大多数属于某一个类别，则该样本也属于该类别。其中k一个是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。 KNN算法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。 KNN算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的模型。k值的选择、距离度量和分类决策规则是KNN算法的三个基本要素。 KNN算法工作原理KNN算法工作原理可描述为：1、假设有一个带有标签的训练样本集，其中包含每条数据与所属分类的对应关系。 2、输入没有带标签的新数据后，计算新数据与训练样本集中每条数据的距离。 3、对求得的所有距离进行升序排序。 4、选取k个与新数据距离最小的训练数据。 5、确定k个训练数据所在类别出现的频率。 6、返回k个训练数据中出现频率最高的类别作为新数据的分类。 KNN算法距离计算方式在KNN中，通过计算对象间距离来作为各个对象之间的相似性指标，这里的距离计算一般采用欧式距离或者是曼哈顿距离，两种距离的计算公式如下图所示： KNN算法特点： 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 适用数据范围：数值型和标称型 KNN算法demo实例python（python3.6）实现一个KNN算法的简单demo 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import numpy as npfrom collections import Counterimport os # 创建样本数据集def createDataSet(): group = np.array([[1.0, 1.1], [1.0, 1.0], [0, 0.1], [0.1, 0]]) labels = ['A', 'A', 'B', 'B'] return group, labels # 近邻算法def classify0 (inX, dataSet, labels, k): diffMat = np.tile(inX,(dataSet.shape[0],1)) - dataSet #待分类的输入向量与每个训练数据做差 distance = ((diffMat ** 2).sum(axis=1)) ** 0.5 #欧氏距离 sortDistanceIndices = distance.argsort() #从小到大的顺序，返回对应索引值 votelabel = [] for i in range(k): votelabel.append(labels[sortDistanceIndices[i]]) Xlabel = Counter(votelabel).most_common(1) return Xlabel[0][0] if __name__ == \"__main__\": # # 创建数据集和 k-近邻算法 group, labels = createDataSet() # 新数据为[0, 0], k=3 label = classify0([0, 0], group, labels, 3) print(label) 执行上面代码，在控制台打印输出 B，即为数据[0, 0]的分类。 方法说明上面python代码中有几个方法在这里简单说明一下： Counter(votelabel).most_common(1)：求votelabel中出现次数最多的元素 np.tile(A,B)：若B为int型：在列方向上将A重复B次 若B为元组(m,n):将A在列方向上重复n次，在行方向上重复m次 sum(axis=1)：函数的axis参数，axis=0:按列相加；axis=1:按行的方向相加，即每行数据求和 argsort：将数组的值按从小到大排序后，输出索引值","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"KNN","slug":"KNN","permalink":"http://yoursite.com/tags/KNN/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"},{"name":"分类算法","slug":"分类算法","permalink":"http://yoursite.com/tags/分类算法/"}]}]}